{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Рекуррентные нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caesar_cipher(text, shift):\n",
    "    result = []\n",
    "    for char in text:\n",
    "        if char.isalpha():  \n",
    "            shift_base = ord('a') \n",
    "            result.append(chr((ord(char) - shift_base + shift) % 26 + shift_base))\n",
    "        else:\n",
    "            result.append(char)  \n",
    "    return ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kosar\\AppData\\Local\\Temp\\ipykernel_18632\\3699413849.py:1: DtypeWarning: Columns (4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/simpsons_script_lines.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/simpsons_script_lines.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = df['normalized_text'].tolist()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig</th>\n",
       "      <th>encrypted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no actually it was a little of both sometimes ...</td>\n",
       "      <td>qr dfwxdoob lw zdv d olwwoh ri erwk vrphwlphv ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wheres mr bergstrom</td>\n",
       "      <td>zkhuhv pu ehujvwurp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i dont know although id sure like to talk to h...</td>\n",
       "      <td>l grqw nqrz dowkrxjk lg vxuh olnh wr wdon wr k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that life is worth living</td>\n",
       "      <td>wkdw olih lv zruwk olylqj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the polls will be open from now until the end ...</td>\n",
       "      <td>wkh sroov zloo eh rshq iurp qrz xqwlo wkh hqg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158266</th>\n",
       "      <td>im back</td>\n",
       "      <td>lp edfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158267</th>\n",
       "      <td>you see class my lyme disease turned out to be</td>\n",
       "      <td>brx vhh fodvv pb obph glvhdvh wxuqhg rxw wr eh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158268</th>\n",
       "      <td>psy-cho-so-ma-tic</td>\n",
       "      <td>svb-fkr-vr-pd-wlf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158269</th>\n",
       "      <td>does that mean you were crazy</td>\n",
       "      <td>grhv wkdw phdq brx zhuh fudcb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158270</th>\n",
       "      <td>no that means she was faking it</td>\n",
       "      <td>qr wkdw phdqv vkh zdv idnlqj lw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158271 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     orig  \\\n",
       "0       no actually it was a little of both sometimes ...   \n",
       "1                                     wheres mr bergstrom   \n",
       "2       i dont know although id sure like to talk to h...   \n",
       "3                               that life is worth living   \n",
       "4       the polls will be open from now until the end ...   \n",
       "...                                                   ...   \n",
       "158266                                            im back   \n",
       "158267     you see class my lyme disease turned out to be   \n",
       "158268                                  psy-cho-so-ma-tic   \n",
       "158269                      does that mean you were crazy   \n",
       "158270                    no that means she was faking it   \n",
       "\n",
       "                                                encrypted  \n",
       "0       qr dfwxdoob lw zdv d olwwoh ri erwk vrphwlphv ...  \n",
       "1                                     zkhuhv pu ehujvwurp  \n",
       "2       l grqw nqrz dowkrxjk lg vxuh olnh wr wdon wr k...  \n",
       "3                               wkdw olih lv zruwk olylqj  \n",
       "4       wkh sroov zloo eh rshq iurp qrz xqwlo wkh hqg ...  \n",
       "...                                                   ...  \n",
       "158266                                            lp edfn  \n",
       "158267     brx vhh fodvv pb obph glvhdvh wxuqhg rxw wr eh  \n",
       "158268                                  svb-fkr-vr-pd-wlf  \n",
       "158269                      grhv wkdw phdq brx zhuh fudcb  \n",
       "158270                    qr wkdw phdqv vkh zdv idnlqj lw  \n",
       "\n",
       "[158271 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_data = {'orig': phrases}\n",
    "df_simpsons = pd.DataFrame(simpsons_data)\n",
    "df_simpsons['orig'] = df_simpsons['orig'].astype(str)\n",
    "\n",
    "shift = 3\n",
    "df_simpsons['encrypted'] = df_simpsons['orig'].apply(lambda x: caesar_cipher(x, shift))\n",
    "\n",
    "df_simpsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(text, char_to_idx):\n",
    "    return [char_to_idx[char] for char in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     orig  \\\n",
      "0       no actually it was a little of both sometimes ...   \n",
      "1                                     wheres mr bergstrom   \n",
      "2       i dont know although id sure like to talk to h...   \n",
      "3                               that life is worth living   \n",
      "4       the polls will be open from now until the end ...   \n",
      "...                                                   ...   \n",
      "158266                                            im back   \n",
      "158267     you see class my lyme disease turned out to be   \n",
      "158268                                  psy-cho-so-ma-tic   \n",
      "158269                      does that mean you were crazy   \n",
      "158270                    no that means she was faking it   \n",
      "\n",
      "                                                encrypted  \\\n",
      "0       qr dfwxdoob lw zdv d olwwoh ri erwk vrphwlphv ...   \n",
      "1                                     zkhuhv pu ehujvwurp   \n",
      "2       l grqw nqrz dowkrxjk lg vxuh olnh wr wdon wr k...   \n",
      "3                               wkdw olih lv zruwk olylqj   \n",
      "4       wkh sroov zloo eh rshq iurp qrz xqwlo wkh hqg ...   \n",
      "...                                                   ...   \n",
      "158266                                            lp edfn   \n",
      "158267     brx vhh fodvv pb obph glvhdvh wxuqhg rxw wr eh   \n",
      "158268                                  svb-fkr-vr-pd-wlf   \n",
      "158269                      grhv wkdw phdq brx zhuh fudcb   \n",
      "158270                    qr wkdw phdqv vkh zdv idnlqj lw   \n",
      "\n",
      "                                            encrypted_seq  \\\n",
      "0       [53, 54, 0, 40, 42, 59, 60, 40, 51, 51, 38, 0,...   \n",
      "1       [62, 47, 44, 57, 44, 58, 0, 52, 57, 0, 41, 44,...   \n",
      "2       [48, 0, 43, 54, 53, 59, 0, 50, 53, 54, 62, 0, ...   \n",
      "3       [59, 47, 40, 59, 0, 51, 48, 45, 44, 0, 48, 58,...   \n",
      "4       [59, 47, 44, 0, 55, 54, 51, 51, 58, 0, 62, 48,...   \n",
      "...                                                   ...   \n",
      "158266                        [48, 52, 0, 41, 40, 42, 50]   \n",
      "158267  [38, 54, 60, 0, 58, 44, 44, 0, 42, 51, 40, 58,...   \n",
      "158268  [55, 58, 38, 5, 42, 47, 54, 5, 58, 54, 5, 52, ...   \n",
      "158269  [43, 54, 44, 58, 0, 59, 47, 40, 59, 0, 52, 44,...   \n",
      "158270  [53, 54, 0, 59, 47, 40, 59, 0, 52, 44, 40, 53,...   \n",
      "\n",
      "                                             original_seq  \n",
      "0       [50, 51, 0, 37, 39, 56, 57, 37, 48, 48, 61, 0,...  \n",
      "1       [59, 44, 41, 54, 41, 55, 0, 49, 54, 0, 38, 41,...  \n",
      "2       [45, 0, 40, 51, 50, 56, 0, 47, 50, 51, 59, 0, ...  \n",
      "3       [56, 44, 37, 56, 0, 48, 45, 42, 41, 0, 45, 55,...  \n",
      "4       [56, 44, 41, 0, 52, 51, 48, 48, 55, 0, 59, 45,...  \n",
      "...                                                   ...  \n",
      "158266                        [45, 49, 0, 38, 37, 39, 47]  \n",
      "158267  [61, 51, 57, 0, 55, 41, 41, 0, 39, 48, 37, 55,...  \n",
      "158268  [52, 55, 61, 5, 39, 44, 51, 5, 55, 51, 5, 49, ...  \n",
      "158269  [40, 51, 41, 55, 0, 56, 44, 37, 56, 0, 49, 41,...  \n",
      "158270  [50, 51, 0, 56, 44, 37, 56, 0, 49, 41, 37, 50,...  \n",
      "\n",
      "[158271 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "unique_chars = set(' '.join(df_simpsons['orig']))\n",
    "char_to_idx = {char: idx for idx, char in enumerate(sorted(unique_chars))}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "char_to_idx['<PAD>'] = len(char_to_idx)  \n",
    "idx_to_char[len(idx_to_char)] = '<PAD>'\n",
    "\n",
    "df_simpsons['encrypted_seq'] = df_simpsons['encrypted'].apply(lambda x: text_to_sequence(x, char_to_idx))\n",
    "df_simpsons['original_seq'] = df_simpsons['orig'].apply(lambda x: text_to_sequence(x, char_to_idx))\n",
    "\n",
    "print(df_simpsons[['orig', 'encrypted', 'encrypted_seq', 'original_seq']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_simpsons, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaesarDataset(Dataset):\n",
    "    def __init__(self, data, char_to_idx):\n",
    "        self.data = data\n",
    "        self.char_to_idx = char_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encrypted_seq = self.data.iloc[idx]['encrypted_seq']\n",
    "        original_seq = self.data.iloc[idx]['original_seq']\n",
    "        return torch.tensor(encrypted_seq, dtype=torch.long), torch.tensor(original_seq, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(batch):\n",
    "    encrypted_seqs = [torch.tensor(item[0], dtype=torch.long) for item in batch]\n",
    "    original_seqs = [torch.tensor(item[1], dtype=torch.long) for item in batch]\n",
    "    \n",
    "    padded_encrypted = pad_sequence(encrypted_seqs, batch_first=True, padding_value=char_to_idx['<PAD>'])\n",
    "    padded_original = pad_sequence(original_seqs, batch_first=True, padding_value=char_to_idx['<PAD>'])\n",
    "    \n",
    "    return padded_encrypted, padded_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CaesarDataset(train, char_to_idx)\n",
    "test_dataset = CaesarDataset(test, char_to_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=pad_sequences)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=pad_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaesarRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(CaesarRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  \n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True) \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) \n",
    "        output, hidden = self.rnn(x) \n",
    "        output = self.fc(output)  \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(char_to_idx)\n",
    "\n",
    "embed_dim = 64\n",
    "hidden_dim = 128\n",
    "\n",
    "model = CaesarRNN(vocab_size, embed_dim, hidden_dim)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=char_to_idx['<PAD>'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for encrypted_seq, original_seq in train_loader:\n",
    "            encrypted_seq = encrypted_seq.to(device)\n",
    "            original_seq = original_seq.to(device)\n",
    "\n",
    "            output = model(encrypted_seq)\n",
    "            \n",
    "            output = output.view(-1, output.size(-1))  \n",
    "            original_seq = original_seq.view(-1)  \n",
    "\n",
    "            loss = criterion(output, original_seq)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for encrypted_seq, original_seq in test_loader:\n",
    "                encrypted_seq = encrypted_seq.to(device)\n",
    "                original_seq = original_seq.to(device)\n",
    "\n",
    "                output = model(encrypted_seq)\n",
    "\n",
    "                output = output.view(-1, output.size(-1))\n",
    "                original_seq = original_seq.view(-1)\n",
    "\n",
    "                loss = criterion(output, original_seq)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        print(f\"Ep [{epoch+1}/{num_epochs}], Time: {epoch_time:.2f} sec, Train Loss: {train_loss/len(train_loader):.4f}, Test Loss: {test_loss/len(test_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kosar\\AppData\\Local\\Temp\\ipykernel_18632\\326600461.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encrypted_seqs = [torch.tensor(item[0], dtype=torch.long) for item in batch]\n",
      "C:\\Users\\kosar\\AppData\\Local\\Temp\\ipykernel_18632\\326600461.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  original_seqs = [torch.tensor(item[1], dtype=torch.long) for item in batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep [1/10], Time: 13.81 sec, Train Loss: 0.0667, Test Loss: 0.0013\n",
      "Ep [2/10], Time: 13.77 sec, Train Loss: 0.0009, Test Loss: 0.0008\n",
      "Ep [3/10], Time: 13.81 sec, Train Loss: 0.0006, Test Loss: 0.0007\n",
      "Ep [4/10], Time: 14.07 sec, Train Loss: 0.0005, Test Loss: 0.0006\n",
      "Ep [5/10], Time: 14.07 sec, Train Loss: 0.0004, Test Loss: 0.0005\n",
      "Ep [6/10], Time: 14.02 sec, Train Loss: 0.0004, Test Loss: 0.0005\n",
      "Ep [7/10], Time: 14.01 sec, Train Loss: 0.0003, Test Loss: 0.0005\n",
      "Ep [8/10], Time: 13.46 sec, Train Loss: 0.0003, Test Loss: 0.0005\n",
      "Ep [9/10], Time: 13.57 sec, Train Loss: 0.0002, Test Loss: 0.0005\n",
      "Ep [10/10], Time: 13.52 sec, Train Loss: 0.0002, Test Loss: 0.0005\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_phrase(model, phrase, char_to_idx, idx_to_char, device):\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        input_seq = torch.tensor([char_to_idx[char] for char in phrase], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "        output = model(input_seq)\n",
    "        \n",
    "        predicted_indices = torch.argmax(output, dim=2).squeeze(0).tolist()\n",
    "\n",
    "        decoded_phrase = ''.join([idx_to_char[idx] for idx in predicted_indices])\n",
    "        return decoded_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сдвиг: 3\n",
      "Зашифрованная фраза: khoor zruog\n",
      "Дешифрованная фраза: hello world\n"
     ]
    }
   ],
   "source": [
    "encrypted_phrase = \"khoor zruog\"  \n",
    "\n",
    "decoded_phrase = decode_phrase(model, encrypted_phrase, char_to_idx, idx_to_char, device)\n",
    "print(f\"Сдвиг: {shift}\")\n",
    "print(f\"Зашифрованная фраза: {encrypted_phrase}\")\n",
    "print(f\"Дешифрованная фраза: {decoded_phrase}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kosar\\AppData\\Local\\Temp\\ipykernel_18632\\1702056208.py:1: DtypeWarning: Columns (4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/simpsons_script_lines.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>timestamp_in_ms</th>\n",
       "      <th>speaking_line</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9549</td>\n",
       "      <td>32</td>\n",
       "      <td>209</td>\n",
       "      <td>Miss Hoover: No, actually, it was a little of ...</td>\n",
       "      <td>848000</td>\n",
       "      <td>True</td>\n",
       "      <td>464.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "      <td>no actually it was a little of both sometimes ...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9550</td>\n",
       "      <td>32</td>\n",
       "      <td>210</td>\n",
       "      <td>Lisa Simpson: (NEAR TEARS) Where's Mr. Bergstrom?</td>\n",
       "      <td>856000</td>\n",
       "      <td>True</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "      <td>wheres mr bergstrom</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9551</td>\n",
       "      <td>32</td>\n",
       "      <td>211</td>\n",
       "      <td>Miss Hoover: I don't know. Although I'd sure l...</td>\n",
       "      <td>856000</td>\n",
       "      <td>True</td>\n",
       "      <td>464.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "      <td>i dont know although id sure like to talk to h...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9552</td>\n",
       "      <td>32</td>\n",
       "      <td>212</td>\n",
       "      <td>Lisa Simpson: That life is worth living.</td>\n",
       "      <td>864000</td>\n",
       "      <td>True</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>That life is worth living.</td>\n",
       "      <td>that life is worth living</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9553</td>\n",
       "      <td>32</td>\n",
       "      <td>213</td>\n",
       "      <td>Edna Krabappel-Flanders: The polls will be ope...</td>\n",
       "      <td>864000</td>\n",
       "      <td>True</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "      <td>the polls will be open from now until the end ...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  episode_id  number  \\\n",
       "0  9549          32     209   \n",
       "1  9550          32     210   \n",
       "2  9551          32     211   \n",
       "3  9552          32     212   \n",
       "4  9553          32     213   \n",
       "\n",
       "                                            raw_text timestamp_in_ms  \\\n",
       "0  Miss Hoover: No, actually, it was a little of ...          848000   \n",
       "1  Lisa Simpson: (NEAR TEARS) Where's Mr. Bergstrom?          856000   \n",
       "2  Miss Hoover: I don't know. Although I'd sure l...          856000   \n",
       "3           Lisa Simpson: That life is worth living.          864000   \n",
       "4  Edna Krabappel-Flanders: The polls will be ope...          864000   \n",
       "\n",
       "  speaking_line character_id  location_id       raw_character_text  \\\n",
       "0          True        464.0          3.0              Miss Hoover   \n",
       "1          True          9.0          3.0             Lisa Simpson   \n",
       "2          True        464.0          3.0              Miss Hoover   \n",
       "3          True          9.0          3.0             Lisa Simpson   \n",
       "4          True         40.0          3.0  Edna Krabappel-Flanders   \n",
       "\n",
       "               raw_location_text  \\\n",
       "0  Springfield Elementary School   \n",
       "1  Springfield Elementary School   \n",
       "2  Springfield Elementary School   \n",
       "3  Springfield Elementary School   \n",
       "4  Springfield Elementary School   \n",
       "\n",
       "                                        spoken_words  \\\n",
       "0  No, actually, it was a little of both. Sometim...   \n",
       "1                             Where's Mr. Bergstrom?   \n",
       "2  I don't know. Although I'd sure like to talk t...   \n",
       "3                         That life is worth living.   \n",
       "4  The polls will be open from now until the end ...   \n",
       "\n",
       "                                     normalized_text word_count  \n",
       "0  no actually it was a little of both sometimes ...         31  \n",
       "1                                wheres mr bergstrom          3  \n",
       "2  i dont know although id sure like to talk to h...         22  \n",
       "3                          that life is worth living          5  \n",
       "4  the polls will be open from now until the end ...         33  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/simpsons_script_lines.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no actually it was a little of both sometimes when a disease is in all the magazines and all the news shows its only natural that you think you have it',\n",
       " 'wheres mr bergstrom',\n",
       " 'i dont know although id sure like to talk to him he didnt touch my lesson plan what did he teach you',\n",
       " 'that life is worth living',\n",
       " 'the polls will be open from now until the end of recess now just in case any of you have decided to put any thought into this well have our final statements martin',\n",
       " 'i dont think theres anything left to say',\n",
       " 'bart',\n",
       " 'victory party under the slide',\n",
       " nan,\n",
       " 'mr bergstrom mr bergstrom']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = df['normalized_text'].tolist()  # колонка с предобработанными текстами\n",
    "phrases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [[c for c in ph] for ph in phrases if type(ph) is str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создаем массив с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS = set('abcdefghijklmnopqrstuvwxyz ')  # все символы, которые мы хотим использовать для кодировки = наш словарь\n",
    "INDEX_TO_CHAR = ['none'] + [w for w in CHARS]  # все неизвестные символы будут получать тег none\n",
    "CHAR_TO_INDEX = {w: i for i, w in enumerate(INDEX_TO_CHAR)}  # словарь токен-индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(INDEX_TO_CHAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50  # мы хотим ограничить максимальную длину ввода\n",
    "X = torch.zeros((len(text), MAX_LEN), dtype=int)  # создаём пустой вектор для текста, чтобы класть в него индексы токенов\n",
    "for i in range(len(text)):  # для каждого предложения\n",
    "    for j, w in enumerate(text[i]):  # для каждого токена\n",
    "        if j >= MAX_LEN:\n",
    "            break\n",
    "        X[i, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX['none'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6, 21, 11, 16,  5,  1, 14, 16,  7,  7, 13, 11, 27,  1, 11, 17, 16, 20,\n",
       "         11, 16, 11,  7, 27,  1,  1,  7, 19, 11, 21,  9, 11, 15, 21,  1, 12, 11,\n",
       "         20, 21, 23, 19,  1, 27, 23, 19, 20, 11, 17, 12, 19,  6],\n",
       "        [17, 12, 19, 18, 19, 20, 11, 23, 18, 11, 15, 19, 18, 22, 20,  1, 18, 21,\n",
       "         23,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [27, 11,  2, 21,  6,  1, 11,  3,  6, 21, 17, 11, 16,  7,  1, 12, 21, 14,\n",
       "         22, 12, 11, 27,  2, 11, 20, 14, 18, 19, 11,  7, 27,  3, 19, 11,  1, 21,\n",
       "         11,  1, 16,  7,  3, 11,  1, 21, 11, 12, 27, 23, 11, 12],\n",
       "        [ 1, 12, 16,  1, 11,  7, 27,  9, 19, 11, 27, 20, 11, 17, 21, 18,  1, 12,\n",
       "         11,  7, 27, 26, 27,  6, 22,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1, 12, 19, 11, 25, 21,  7,  7, 20, 11, 17, 27,  7,  7, 11, 15, 19, 11,\n",
       "         21, 25, 19,  6, 11,  9, 18, 21, 23, 11,  6, 21, 17, 11, 14,  6,  1, 27,\n",
       "          7, 11,  1, 12, 19, 11, 19,  6,  2, 11, 21,  9, 11, 18]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding и RNN ячейки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 50])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 50, 28])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = torch.nn.Embedding(len(INDEX_TO_CHAR), 28)  # размер словаря * размер вектора для кодировки каждого слова\n",
    "t = embeddings(X[0:5])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 50, 28]), torch.Size([5, 50]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape, X[0:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 50, 128]), torch.Size([1, 5, 128]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = torch.nn.RNN(28, 128, batch_first=True)  # на вход - размер эмбеддинга, размер скрытого состояния и порядок размерностей\n",
    "o, s = rnn(t)\n",
    "# вектора для слов: батч * число токенов * размер скрытого состояния\n",
    "# вектор скрытого состояния: число вектров (один) * батч * размер скрытого состояния\n",
    "o.shape, s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 50, 128]), torch.Size([1, 5, 128]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o, s2 = rnn(t, s)\n",
    "o.shape, s2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализация сети с RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(28, 30)\n",
    "        self.rnn = torch.nn.RNN(30, 128)\n",
    "        self.out = torch.nn.Linear(128, 28)\n",
    "\n",
    "    def forward(self, sentences, state=None):\n",
    "        x = self.embedding(sentences)\n",
    "        x, s = self.rnn(x) # берём выход с последнего слоя для всех токенов, а не скрытое состояние\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()  # типичный лосс многоклассовой классификации\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 57.233, Train loss: 1.827\n",
      "Epoch 1. Time: 59.447, Train loss: 1.729\n",
      "Epoch 2. Time: 60.259, Train loss: 1.713\n",
      "Epoch 3. Time: 59.961, Train loss: 1.703\n",
      "Epoch 4. Time: 59.517, Train loss: 1.695\n",
      "Epoch 5. Time: 59.810, Train loss: 1.688\n",
      "Epoch 6. Time: 59.438, Train loss: 1.684\n",
      "Epoch 7. Time: 60.340, Train loss: 1.681\n",
      "Epoch 8. Time: 60.055, Train loss: 1.678\n",
      "Epoch 9. Time: 59.759, Train loss: 1.675\n",
      "Epoch 10. Time: 59.697, Train loss: 1.673\n",
      "Epoch 11. Time: 59.851, Train loss: 1.671\n",
      "Epoch 12. Time: 59.993, Train loss: 1.670\n",
      "Epoch 13. Time: 59.510, Train loss: 1.668\n",
      "Epoch 14. Time: 59.439, Train loss: 1.667\n",
      "Epoch 15. Time: 60.134, Train loss: 1.665\n",
      "Epoch 16. Time: 59.893, Train loss: 1.668\n",
      "Epoch 17. Time: 59.521, Train loss: 1.666\n",
      "Epoch 18. Time: 58.612, Train loss: 1.664\n",
      "Epoch 19. Time: 60.518, Train loss: 1.662\n"
     ]
    }
   ],
   "source": [
    "for ep in range(20):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "\n",
    "    for i in range(int(len(X) / 100)):\n",
    "        # берём батч в 100 элементов\n",
    "        batch = X[i * 100:(i + 1) * 100]\n",
    "        X_batch = batch[:, :-1]\n",
    "        Y_batch = batch[:, 1:].flatten()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        answers = model.forward(X_batch)\n",
    "        answers = answers.view(-1, len(INDEX_TO_CHAR))\n",
    "        loss = criterion(answers, Y_batch)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "\n",
    "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генерация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAR_TO_INDEX['none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(word):\n",
    "    sentence = list(word)\n",
    "    sentence = [CHAR_TO_INDEX.get(s, 0) for s in sentence]\n",
    "    answers = model.forward(torch.tensor(sentence))\n",
    "    probas, indices = answers.topk(1)\n",
    "    return ''.join([INDEX_TO_CHAR[ind.item()] for ind in indices.flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' enhil   in '"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence('that life is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'enohueotrte  '"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence('victory party')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mult_nn_net",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
