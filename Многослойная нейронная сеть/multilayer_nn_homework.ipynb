{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision as tv\n",
    "from torchmetrics import Accuracy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Многослойная нейронная сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mnist = tv.datasets.MNIST(\n",
    "    '.',\n",
    "    train=True,\n",
    "    transform=tv.transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_mnist = tv.datasets.MNIST(\n",
    "    '.',\n",
    "    train=False,\n",
    "    transform=tv.transforms.ToTensor(),\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.DataLoader(train_mnist, batch_size=BATCH_SIZE)\n",
    "test = torch.utils.data.DataLoader(test_mnist, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mnist[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mnist[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear_relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(784, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.flatten(X)\n",
    "        logits = self.linear_relu_stack(X)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Использую cuda-устройство\n",
      "\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#device = \"cpu\"\n",
    "print(f\"Использую {device}-устройство\\n\")\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_nn):\n",
    "    train_accuracy = Accuracy(task='multiclass', num_classes=10).to(device)\n",
    "    test_accuracy = Accuracy(task='multiclass', num_classes=10).to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        train_loss = .0 \n",
    "        train_iters = 0\n",
    "        model_nn.train()\n",
    "        for X, y in train:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model_nn(X)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            l = loss(y_pred, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += l.item()\n",
    "            train_iters += 1\n",
    "            train_accuracy.update(y_pred, y)\n",
    "        train_acc_out = train_accuracy.compute()\n",
    "        train_accuracy.reset()\n",
    "\n",
    "        test_loss = .0\n",
    "        test_iters = 0\n",
    "        model_nn.eval()\n",
    "        for X, y in test:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model_nn(X)\n",
    "\n",
    "            l = loss(y_pred, y)\n",
    "\n",
    "            test_loss += l.item()\n",
    "            test_iters += 1\n",
    "            test_accuracy.update(y_pred, y)\n",
    "        test_acc_out = test_accuracy.compute()\n",
    "        test_accuracy.reset()\n",
    "\n",
    "        print(f\"ep: {epoch+1} | time: {time.time()-start:.2f} сек | train_loss: {train_loss/train_iters:.2f} | train_acc: {train_acc_out*100:.2f}%\" \n",
    "              f\" | test_loss: {test_loss/test_iters:.2f} | test_acc: {test_acc_out*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 1 | time: 4.15 сек | train_loss: 2.28 | train_acc: 16.76% | test_loss: 2.25 | test_acc: 24.21%\n",
      "ep: 2 | time: 4.30 сек | train_loss: 2.23 | train_acc: 33.37% | test_loss: 2.20 | test_acc: 41.83%\n",
      "ep: 3 | time: 4.26 сек | train_loss: 2.18 | train_acc: 46.99% | test_loss: 2.15 | test_acc: 52.88%\n",
      "ep: 4 | time: 4.20 сек | train_loss: 2.13 | train_acc: 56.60% | test_loss: 2.09 | test_acc: 61.18%\n",
      "ep: 5 | time: 4.17 сек | train_loss: 2.07 | train_acc: 64.05% | test_loss: 2.03 | test_acc: 66.95%\n",
      "ep: 6 | time: 4.07 сек | train_loss: 2.01 | train_acc: 68.11% | test_loss: 1.97 | test_acc: 70.11%\n",
      "ep: 7 | time: 4.04 сек | train_loss: 1.94 | train_acc: 70.57% | test_loss: 1.89 | test_acc: 72.18%\n",
      "ep: 8 | time: 4.14 сек | train_loss: 1.87 | train_acc: 72.03% | test_loss: 1.82 | test_acc: 73.44%\n",
      "ep: 9 | time: 3.94 сек | train_loss: 1.79 | train_acc: 73.05% | test_loss: 1.74 | test_acc: 74.43%\n",
      "ep: 10 | time: 4.16 сек | train_loss: 1.71 | train_acc: 73.99% | test_loss: 1.66 | test_acc: 75.47%\n"
     ]
    }
   ],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Точность модели на тестовых данных составляет 75.47%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуем заменить SGD на Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkAdam(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetworkAdam, self).__init__()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear_relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(784, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.flatten(X)\n",
    "        logits = self.linear_relu_stack(X)\n",
    "\n",
    "        return logits\n",
    "\n",
    "model_adam = NeuralNetworkAdam().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_adam.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 1 | time: 4.45 сек | train_loss: 0.46 | train_acc: 88.37% | test_loss: 0.24 | test_acc: 93.10%\n",
      "ep: 2 | time: 4.35 сек | train_loss: 0.21 | train_acc: 93.91% | test_loss: 0.17 | test_acc: 95.00%\n",
      "ep: 3 | time: 4.25 сек | train_loss: 0.16 | train_acc: 95.55% | test_loss: 0.13 | test_acc: 96.10%\n",
      "ep: 4 | time: 4.29 сек | train_loss: 0.12 | train_acc: 96.53% | test_loss: 0.11 | test_acc: 96.71%\n",
      "ep: 5 | time: 4.47 сек | train_loss: 0.10 | train_acc: 97.18% | test_loss: 0.10 | test_acc: 97.11%\n",
      "ep: 6 | time: 4.44 сек | train_loss: 0.08 | train_acc: 97.71% | test_loss: 0.09 | test_acc: 97.28%\n",
      "ep: 7 | time: 4.48 сек | train_loss: 0.07 | train_acc: 98.06% | test_loss: 0.09 | test_acc: 97.33%\n",
      "ep: 8 | time: 4.45 сек | train_loss: 0.06 | train_acc: 98.40% | test_loss: 0.08 | test_acc: 97.35%\n",
      "ep: 9 | time: 4.44 сек | train_loss: 0.05 | train_acc: 98.66% | test_loss: 0.08 | test_acc: 97.43%\n",
      "ep: 10 | time: 4.39 сек | train_loss: 0.04 | train_acc: 98.90% | test_loss: 0.08 | test_acc: 97.43%\n"
     ]
    }
   ],
   "source": [
    "train_model(model_adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam дает лучшую оценку на тестовых данных, нежели SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавим больше слоев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkAdamMoreLays(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetworkAdamMoreLays, self).__init__()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear_relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(784, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.flatten(X)\n",
    "        logits = self.linear_relu_stack(X)\n",
    "\n",
    "        return logits\n",
    "\n",
    "model_adam_more_lays = NeuralNetworkAdamMoreLays().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_adam_more_lays.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 1 | time: 4.55 сек | train_loss: 0.42 | train_acc: 87.99% | test_loss: 0.20 | test_acc: 93.53%\n",
      "ep: 2 | time: 4.48 сек | train_loss: 0.15 | train_acc: 95.54% | test_loss: 0.12 | test_acc: 96.22%\n",
      "ep: 3 | time: 4.45 сек | train_loss: 0.09 | train_acc: 97.25% | test_loss: 0.10 | test_acc: 97.03%\n",
      "ep: 4 | time: 4.57 сек | train_loss: 0.07 | train_acc: 98.00% | test_loss: 0.09 | test_acc: 97.03%\n",
      "ep: 5 | time: 4.60 сек | train_loss: 0.05 | train_acc: 98.45% | test_loss: 0.09 | test_acc: 97.27%\n",
      "ep: 6 | time: 4.63 сек | train_loss: 0.04 | train_acc: 98.65% | test_loss: 0.08 | test_acc: 97.56%\n",
      "ep: 7 | time: 4.78 сек | train_loss: 0.04 | train_acc: 98.86% | test_loss: 0.09 | test_acc: 97.65%\n",
      "ep: 8 | time: 4.72 сек | train_loss: 0.03 | train_acc: 99.12% | test_loss: 0.09 | test_acc: 97.45%\n",
      "ep: 9 | time: 4.64 сек | train_loss: 0.02 | train_acc: 99.24% | test_loss: 0.09 | test_acc: 97.49%\n",
      "ep: 10 | time: 4.61 сек | train_loss: 0.02 | train_acc: 99.44% | test_loss: 0.09 | test_acc: 97.64%\n"
     ]
    }
   ],
   "source": [
    "train_model(model_adam_more_lays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Результат еще лучше!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавим BatchNorm-слои"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkBatchNorm(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetworkBatchNorm, self).__init__()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear_relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(784, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(256),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.flatten(X)\n",
    "        logits = self.linear_relu_stack(X)\n",
    "\n",
    "        return logits\n",
    "\n",
    "model_batchnorm = NeuralNetworkBatchNorm().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_batchnorm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 1 | time: 4.61 сек | train_loss: 0.20 | train_acc: 94.31% | test_loss: 0.11 | test_acc: 96.39%\n",
      "ep: 2 | time: 4.50 сек | train_loss: 0.08 | train_acc: 97.72% | test_loss: 0.10 | test_acc: 96.86%\n",
      "ep: 3 | time: 4.62 сек | train_loss: 0.05 | train_acc: 98.57% | test_loss: 0.08 | test_acc: 97.55%\n",
      "ep: 4 | time: 4.57 сек | train_loss: 0.03 | train_acc: 99.01% | test_loss: 0.09 | test_acc: 97.12%\n",
      "ep: 5 | time: 4.67 сек | train_loss: 0.03 | train_acc: 99.16% | test_loss: 0.10 | test_acc: 97.02%\n",
      "ep: 6 | time: 4.69 сек | train_loss: 0.02 | train_acc: 99.22% | test_loss: 0.09 | test_acc: 97.27%\n",
      "ep: 7 | time: 4.74 сек | train_loss: 0.02 | train_acc: 99.43% | test_loss: 0.10 | test_acc: 97.54%\n",
      "ep: 8 | time: 4.55 сек | train_loss: 0.01 | train_acc: 99.64% | test_loss: 0.09 | test_acc: 97.63%\n",
      "ep: 9 | time: 4.71 сек | train_loss: 0.01 | train_acc: 99.54% | test_loss: 0.08 | test_acc: 97.71%\n",
      "ep: 10 | time: 4.65 сек | train_loss: 0.01 | train_acc: 99.61% | test_loss: 0.08 | test_acc: 97.93%\n"
     ]
    }
   ],
   "source": [
    "train_model(model_batchnorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### С батч-нормализацией модель имеет наилучший результат на тестовых данных!!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ради интереса попробуем Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkDropout(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetworkDropout, self).__init__()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear_relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(784, 2560),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(2560, 1280),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(1280, 640),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(640, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.flatten(X)\n",
    "        logits = self.linear_relu_stack(X)\n",
    "\n",
    "        return logits\n",
    "\n",
    "model_dropout = NeuralNetworkDropout().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_dropout.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 1 | time: 4.41 сек | train_loss: 0.33 | train_acc: 89.90% | test_loss: 0.16 | test_acc: 94.67%\n",
      "ep: 2 | time: 4.77 сек | train_loss: 0.14 | train_acc: 95.93% | test_loss: 0.11 | test_acc: 96.74%\n",
      "ep: 3 | time: 4.87 сек | train_loss: 0.11 | train_acc: 96.72% | test_loss: 0.10 | test_acc: 96.87%\n",
      "ep: 4 | time: 4.79 сек | train_loss: 0.09 | train_acc: 97.35% | test_loss: 0.08 | test_acc: 97.55%\n",
      "ep: 5 | time: 4.93 сек | train_loss: 0.08 | train_acc: 97.61% | test_loss: 0.08 | test_acc: 97.60%\n",
      "ep: 6 | time: 5.06 сек | train_loss: 0.07 | train_acc: 97.88% | test_loss: 0.08 | test_acc: 97.84%\n",
      "ep: 7 | time: 5.14 сек | train_loss: 0.07 | train_acc: 97.99% | test_loss: 0.08 | test_acc: 97.81%\n",
      "ep: 8 | time: 4.81 сек | train_loss: 0.06 | train_acc: 98.26% | test_loss: 0.08 | test_acc: 97.89%\n",
      "ep: 9 | time: 4.97 сек | train_loss: 0.06 | train_acc: 98.23% | test_loss: 0.07 | test_acc: 98.08%\n",
      "ep: 10 | time: 5.03 сек | train_loss: 0.05 | train_acc: 98.46% | test_loss: 0.07 | test_acc: 98.10%\n"
     ]
    }
   ],
   "source": [
    "train_model(model_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### При шаге обучения 0.001 модель показывает хороший результат. Если повысить шаг до 0.01, то оценка на тестовых данных будет гораздо лучше, чем на обучающих"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mult_nn_net",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
