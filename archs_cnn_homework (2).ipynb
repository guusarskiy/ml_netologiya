{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision as tv\n",
    "from torchsummary import summary\n",
    "import time\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_losses = []\n",
    "dense_losses = []\n",
    "vgg_losses = []\n",
    "inception_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0, 0\n",
    "    net.eval()\n",
    "    for X, y in data_iter:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum.item() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, trainer, num_epochs, list_of_losses):\n",
    "    net.to(device)\n",
    "    loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        \n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            trainer.zero_grad()\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "\n",
    "            if i % 10 == 0:\n",
    "              print(f\"Step {i}. time since epoch: {time.time() -  start:.3f}. \" \n",
    "                    f\"Train acc: {train_acc_sum / n:.3f}. Train Loss: {train_l_sum / n:.3f}\")\n",
    "            list_of_losses.append(round(train_l_sum / n, 3))\n",
    "        test_acc = evaluate_accuracy(test_iter, net.to(device))\n",
    "        print('-' * 20)\n",
    "        print(f'epoch {epoch + 1}, loss {train_l_sum / n:.4f}, train acc {train_acc_sum / n:.3f}'\n",
    "              f', test acc {test_acc:.3f}, time {time.time() - start:.1f} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "transoforms = tv.transforms.Compose([\n",
    "    tv.transforms.Grayscale(num_output_channels=3),\n",
    "    tv.transforms.Resize((224, 224)),\n",
    "    tv.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = tv.datasets.EMNIST('.', split='byclass', train=True, transform=transoforms, download=True)\n",
    "test_dataset = tv.datasets.EMNIST('.', split='byclass', train=False, transform=transoforms, download=True)\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tv.models.densenet161(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Linear(in_features=2208, out_features=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.weight\n",
      "\t classifier.bias\n"
     ]
    }
   ],
   "source": [
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.Adam(params_to_update, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0. time since epoch: 1.255. Train acc: 0.012. Train Loss: 4.274\n",
      "Step 10. time since epoch: 10.285. Train acc: 0.134. Train Loss: 3.638\n",
      "Step 20. time since epoch: 19.093. Train acc: 0.236. Train Loss: 3.319\n",
      "Step 30. time since epoch: 28.105. Train acc: 0.301. Train Loss: 3.102\n",
      "Step 40. time since epoch: 36.971. Train acc: 0.345. Train Loss: 2.918\n",
      "Step 50. time since epoch: 45.835. Train acc: 0.379. Train Loss: 2.765\n",
      "Step 60. time since epoch: 54.750. Train acc: 0.408. Train Loss: 2.634\n",
      "Step 70. time since epoch: 63.591. Train acc: 0.433. Train Loss: 2.516\n",
      "Step 80. time since epoch: 72.582. Train acc: 0.453. Train Loss: 2.413\n",
      "Step 90. time since epoch: 81.449. Train acc: 0.471. Train Loss: 2.325\n",
      "Step 100. time since epoch: 90.245. Train acc: 0.486. Train Loss: 2.251\n",
      "Step 110. time since epoch: 99.055. Train acc: 0.498. Train Loss: 2.187\n",
      "Step 120. time since epoch: 107.835. Train acc: 0.511. Train Loss: 2.123\n",
      "Step 130. time since epoch: 116.662. Train acc: 0.523. Train Loss: 2.065\n",
      "Step 140. time since epoch: 125.817. Train acc: 0.532. Train Loss: 2.018\n",
      "Step 150. time since epoch: 134.581. Train acc: 0.540. Train Loss: 1.974\n",
      "Step 160. time since epoch: 143.398. Train acc: 0.549. Train Loss: 1.930\n",
      "Step 170. time since epoch: 152.131. Train acc: 0.557. Train Loss: 1.887\n",
      "Step 180. time since epoch: 160.941. Train acc: 0.565. Train Loss: 1.849\n",
      "Step 190. time since epoch: 169.689. Train acc: 0.571. Train Loss: 1.815\n",
      "Step 200. time since epoch: 178.427. Train acc: 0.577. Train Loss: 1.782\n",
      "Step 210. time since epoch: 187.236. Train acc: 0.582. Train Loss: 1.753\n",
      "Step 220. time since epoch: 195.967. Train acc: 0.587. Train Loss: 1.726\n",
      "Step 230. time since epoch: 204.776. Train acc: 0.592. Train Loss: 1.701\n",
      "Step 240. time since epoch: 213.546. Train acc: 0.597. Train Loss: 1.678\n",
      "Step 250. time since epoch: 222.384. Train acc: 0.602. Train Loss: 1.655\n",
      "Step 260. time since epoch: 231.148. Train acc: 0.606. Train Loss: 1.633\n",
      "Step 270. time since epoch: 239.935. Train acc: 0.609. Train Loss: 1.613\n",
      "Step 280. time since epoch: 248.758. Train acc: 0.614. Train Loss: 1.592\n",
      "Step 290. time since epoch: 257.552. Train acc: 0.618. Train Loss: 1.573\n",
      "Step 300. time since epoch: 266.340. Train acc: 0.621. Train Loss: 1.555\n",
      "Step 310. time since epoch: 275.067. Train acc: 0.625. Train Loss: 1.538\n",
      "Step 320. time since epoch: 283.852. Train acc: 0.628. Train Loss: 1.521\n",
      "Step 330. time since epoch: 292.570. Train acc: 0.631. Train Loss: 1.504\n",
      "Step 340. time since epoch: 301.419. Train acc: 0.634. Train Loss: 1.489\n",
      "Step 350. time since epoch: 310.100. Train acc: 0.637. Train Loss: 1.475\n",
      "Step 360. time since epoch: 318.886. Train acc: 0.639. Train Loss: 1.461\n",
      "Step 370. time since epoch: 327.590. Train acc: 0.642. Train Loss: 1.448\n",
      "Step 380. time since epoch: 336.344. Train acc: 0.645. Train Loss: 1.434\n",
      "Step 390. time since epoch: 345.045. Train acc: 0.647. Train Loss: 1.421\n",
      "Step 400. time since epoch: 353.848. Train acc: 0.649. Train Loss: 1.409\n",
      "Step 410. time since epoch: 362.546. Train acc: 0.651. Train Loss: 1.398\n",
      "Step 420. time since epoch: 371.309. Train acc: 0.653. Train Loss: 1.387\n",
      "Step 430. time since epoch: 379.978. Train acc: 0.655. Train Loss: 1.376\n",
      "Step 440. time since epoch: 388.689. Train acc: 0.657. Train Loss: 1.367\n",
      "Step 450. time since epoch: 397.433. Train acc: 0.659. Train Loss: 1.357\n",
      "Step 460. time since epoch: 406.135. Train acc: 0.661. Train Loss: 1.347\n",
      "Step 470. time since epoch: 415.020. Train acc: 0.663. Train Loss: 1.337\n",
      "Step 480. time since epoch: 423.878. Train acc: 0.665. Train Loss: 1.327\n",
      "Step 490. time since epoch: 432.631. Train acc: 0.667. Train Loss: 1.318\n",
      "Step 500. time since epoch: 441.344. Train acc: 0.668. Train Loss: 1.309\n",
      "Step 510. time since epoch: 450.197. Train acc: 0.670. Train Loss: 1.300\n",
      "Step 520. time since epoch: 459.077. Train acc: 0.671. Train Loss: 1.293\n",
      "Step 530. time since epoch: 467.891. Train acc: 0.673. Train Loss: 1.285\n",
      "Step 540. time since epoch: 476.619. Train acc: 0.675. Train Loss: 1.276\n",
      "Step 550. time since epoch: 485.358. Train acc: 0.676. Train Loss: 1.268\n",
      "Step 560. time since epoch: 494.152. Train acc: 0.678. Train Loss: 1.260\n",
      "Step 570. time since epoch: 502.944. Train acc: 0.679. Train Loss: 1.253\n",
      "Step 580. time since epoch: 511.696. Train acc: 0.680. Train Loss: 1.246\n",
      "Step 590. time since epoch: 520.461. Train acc: 0.682. Train Loss: 1.239\n",
      "Step 600. time since epoch: 529.224. Train acc: 0.683. Train Loss: 1.232\n",
      "Step 610. time since epoch: 538.059. Train acc: 0.684. Train Loss: 1.225\n",
      "Step 620. time since epoch: 546.852. Train acc: 0.686. Train Loss: 1.219\n",
      "Step 630. time since epoch: 555.713. Train acc: 0.687. Train Loss: 1.212\n",
      "Step 640. time since epoch: 564.516. Train acc: 0.688. Train Loss: 1.206\n",
      "Step 650. time since epoch: 573.356. Train acc: 0.689. Train Loss: 1.200\n",
      "Step 660. time since epoch: 582.117. Train acc: 0.690. Train Loss: 1.194\n",
      "Step 670. time since epoch: 590.854. Train acc: 0.691. Train Loss: 1.188\n",
      "Step 680. time since epoch: 599.614. Train acc: 0.692. Train Loss: 1.183\n",
      "Step 690. time since epoch: 608.351. Train acc: 0.694. Train Loss: 1.177\n",
      "Step 700. time since epoch: 617.113. Train acc: 0.695. Train Loss: 1.171\n",
      "Step 710. time since epoch: 625.887. Train acc: 0.696. Train Loss: 1.167\n",
      "Step 720. time since epoch: 634.612. Train acc: 0.697. Train Loss: 1.161\n",
      "Step 730. time since epoch: 643.361. Train acc: 0.697. Train Loss: 1.157\n",
      "Step 740. time since epoch: 652.175. Train acc: 0.698. Train Loss: 1.152\n",
      "Step 750. time since epoch: 660.958. Train acc: 0.700. Train Loss: 1.146\n",
      "Step 760. time since epoch: 669.634. Train acc: 0.700. Train Loss: 1.142\n",
      "Step 770. time since epoch: 678.412. Train acc: 0.701. Train Loss: 1.137\n",
      "Step 780. time since epoch: 687.199. Train acc: 0.702. Train Loss: 1.132\n",
      "Step 790. time since epoch: 695.988. Train acc: 0.703. Train Loss: 1.127\n",
      "Step 800. time since epoch: 704.782. Train acc: 0.704. Train Loss: 1.123\n",
      "Step 810. time since epoch: 713.505. Train acc: 0.705. Train Loss: 1.118\n",
      "Step 820. time since epoch: 722.310. Train acc: 0.706. Train Loss: 1.114\n",
      "Step 830. time since epoch: 730.975. Train acc: 0.707. Train Loss: 1.110\n",
      "Step 840. time since epoch: 739.782. Train acc: 0.708. Train Loss: 1.106\n",
      "Step 850. time since epoch: 748.502. Train acc: 0.709. Train Loss: 1.101\n",
      "Step 860. time since epoch: 757.247. Train acc: 0.709. Train Loss: 1.097\n",
      "Step 870. time since epoch: 766.038. Train acc: 0.710. Train Loss: 1.093\n",
      "Step 880. time since epoch: 774.732. Train acc: 0.710. Train Loss: 1.089\n",
      "Step 890. time since epoch: 783.490. Train acc: 0.711. Train Loss: 1.085\n",
      "Step 900. time since epoch: 792.262. Train acc: 0.712. Train Loss: 1.082\n",
      "Step 910. time since epoch: 800.951. Train acc: 0.713. Train Loss: 1.078\n",
      "Step 920. time since epoch: 809.686. Train acc: 0.713. Train Loss: 1.075\n",
      "Step 930. time since epoch: 818.444. Train acc: 0.714. Train Loss: 1.071\n",
      "Step 940. time since epoch: 827.190. Train acc: 0.715. Train Loss: 1.068\n",
      "Step 950. time since epoch: 835.950. Train acc: 0.716. Train Loss: 1.064\n",
      "Step 960. time since epoch: 844.713. Train acc: 0.716. Train Loss: 1.061\n",
      "Step 970. time since epoch: 853.463. Train acc: 0.717. Train Loss: 1.058\n",
      "Step 980. time since epoch: 862.291. Train acc: 0.717. Train Loss: 1.054\n",
      "Step 990. time since epoch: 871.084. Train acc: 0.718. Train Loss: 1.052\n",
      "Step 1000. time since epoch: 879.853. Train acc: 0.718. Train Loss: 1.048\n",
      "Step 1010. time since epoch: 888.651. Train acc: 0.719. Train Loss: 1.045\n",
      "Step 1020. time since epoch: 897.485. Train acc: 0.719. Train Loss: 1.043\n",
      "Step 1030. time since epoch: 906.242. Train acc: 0.720. Train Loss: 1.039\n",
      "Step 1040. time since epoch: 915.065. Train acc: 0.720. Train Loss: 1.036\n",
      "Step 1050. time since epoch: 923.868. Train acc: 0.721. Train Loss: 1.033\n",
      "Step 1060. time since epoch: 932.652. Train acc: 0.722. Train Loss: 1.030\n",
      "Step 1070. time since epoch: 941.487. Train acc: 0.722. Train Loss: 1.027\n",
      "Step 1080. time since epoch: 950.373. Train acc: 0.723. Train Loss: 1.025\n",
      "Step 1090. time since epoch: 959.096. Train acc: 0.723. Train Loss: 1.022\n",
      "Step 1100. time since epoch: 967.914. Train acc: 0.724. Train Loss: 1.020\n",
      "Step 1110. time since epoch: 976.642. Train acc: 0.724. Train Loss: 1.017\n",
      "Step 1120. time since epoch: 985.497. Train acc: 0.725. Train Loss: 1.014\n",
      "Step 1130. time since epoch: 994.236. Train acc: 0.725. Train Loss: 1.011\n",
      "Step 1140. time since epoch: 1003.066. Train acc: 0.725. Train Loss: 1.009\n",
      "Step 1150. time since epoch: 1011.852. Train acc: 0.726. Train Loss: 1.006\n",
      "Step 1160. time since epoch: 1020.591. Train acc: 0.726. Train Loss: 1.003\n",
      "Step 1170. time since epoch: 1029.315. Train acc: 0.727. Train Loss: 1.001\n",
      "Step 1180. time since epoch: 1037.991. Train acc: 0.727. Train Loss: 0.998\n",
      "Step 1190. time since epoch: 1046.781. Train acc: 0.728. Train Loss: 0.996\n",
      "Step 1200. time since epoch: 1055.525. Train acc: 0.728. Train Loss: 0.994\n",
      "Step 1210. time since epoch: 1064.281. Train acc: 0.729. Train Loss: 0.991\n",
      "Step 1220. time since epoch: 1073.026. Train acc: 0.729. Train Loss: 0.989\n",
      "Step 1230. time since epoch: 1081.745. Train acc: 0.730. Train Loss: 0.986\n",
      "Step 1240. time since epoch: 1090.469. Train acc: 0.730. Train Loss: 0.984\n",
      "Step 1250. time since epoch: 1099.218. Train acc: 0.731. Train Loss: 0.982\n",
      "Step 1260. time since epoch: 1107.967. Train acc: 0.731. Train Loss: 0.979\n",
      "Step 1270. time since epoch: 1116.759. Train acc: 0.732. Train Loss: 0.977\n",
      "Step 1280. time since epoch: 1125.462. Train acc: 0.732. Train Loss: 0.975\n",
      "Step 1290. time since epoch: 1134.213. Train acc: 0.732. Train Loss: 0.973\n",
      "Step 1300. time since epoch: 1142.988. Train acc: 0.733. Train Loss: 0.971\n",
      "Step 1310. time since epoch: 1151.708. Train acc: 0.733. Train Loss: 0.969\n",
      "Step 1320. time since epoch: 1160.463. Train acc: 0.734. Train Loss: 0.967\n",
      "Step 1330. time since epoch: 1169.227. Train acc: 0.734. Train Loss: 0.965\n",
      "Step 1340. time since epoch: 1177.986. Train acc: 0.735. Train Loss: 0.962\n",
      "Step 1350. time since epoch: 1186.774. Train acc: 0.735. Train Loss: 0.961\n",
      "Step 1360. time since epoch: 1195.506. Train acc: 0.735. Train Loss: 0.959\n",
      "Step 1370. time since epoch: 1204.316. Train acc: 0.736. Train Loss: 0.957\n",
      "Step 1380. time since epoch: 1213.049. Train acc: 0.736. Train Loss: 0.955\n",
      "Step 1390. time since epoch: 1221.809. Train acc: 0.736. Train Loss: 0.953\n",
      "Step 1400. time since epoch: 1230.550. Train acc: 0.737. Train Loss: 0.951\n",
      "Step 1410. time since epoch: 1239.285. Train acc: 0.737. Train Loss: 0.949\n",
      "Step 1420. time since epoch: 1248.067. Train acc: 0.737. Train Loss: 0.948\n",
      "Step 1430. time since epoch: 1256.824. Train acc: 0.738. Train Loss: 0.946\n",
      "Step 1440. time since epoch: 1265.660. Train acc: 0.738. Train Loss: 0.944\n",
      "Step 1450. time since epoch: 1274.360. Train acc: 0.739. Train Loss: 0.942\n",
      "Step 1460. time since epoch: 1283.145. Train acc: 0.739. Train Loss: 0.940\n",
      "Step 1470. time since epoch: 1291.972. Train acc: 0.739. Train Loss: 0.939\n",
      "Step 1480. time since epoch: 1300.833. Train acc: 0.740. Train Loss: 0.937\n",
      "Step 1490. time since epoch: 1309.614. Train acc: 0.740. Train Loss: 0.935\n",
      "Step 1500. time since epoch: 1318.414. Train acc: 0.740. Train Loss: 0.934\n",
      "Step 1510. time since epoch: 1327.180. Train acc: 0.741. Train Loss: 0.932\n",
      "Step 1520. time since epoch: 1335.958. Train acc: 0.741. Train Loss: 0.930\n",
      "Step 1530. time since epoch: 1344.762. Train acc: 0.741. Train Loss: 0.929\n",
      "Step 1540. time since epoch: 1353.528. Train acc: 0.741. Train Loss: 0.927\n",
      "Step 1550. time since epoch: 1362.285. Train acc: 0.742. Train Loss: 0.926\n",
      "Step 1560. time since epoch: 1371.074. Train acc: 0.742. Train Loss: 0.924\n",
      "Step 1570. time since epoch: 1379.822. Train acc: 0.742. Train Loss: 0.923\n",
      "Step 1580. time since epoch: 1388.589. Train acc: 0.743. Train Loss: 0.921\n",
      "Step 1590. time since epoch: 1397.349. Train acc: 0.743. Train Loss: 0.920\n",
      "Step 1600. time since epoch: 1406.083. Train acc: 0.743. Train Loss: 0.918\n",
      "Step 1610. time since epoch: 1414.828. Train acc: 0.744. Train Loss: 0.917\n",
      "Step 1620. time since epoch: 1423.574. Train acc: 0.744. Train Loss: 0.915\n",
      "Step 1630. time since epoch: 1432.237. Train acc: 0.744. Train Loss: 0.913\n",
      "Step 1640. time since epoch: 1440.936. Train acc: 0.745. Train Loss: 0.912\n",
      "Step 1650. time since epoch: 1449.673. Train acc: 0.745. Train Loss: 0.910\n",
      "Step 1660. time since epoch: 1458.413. Train acc: 0.745. Train Loss: 0.909\n",
      "Step 1670. time since epoch: 1467.105. Train acc: 0.745. Train Loss: 0.907\n",
      "Step 1680. time since epoch: 1475.812. Train acc: 0.746. Train Loss: 0.906\n",
      "Step 1690. time since epoch: 1484.509. Train acc: 0.746. Train Loss: 0.905\n",
      "Step 1700. time since epoch: 1493.287. Train acc: 0.746. Train Loss: 0.903\n",
      "Step 1710. time since epoch: 1501.998. Train acc: 0.746. Train Loss: 0.902\n",
      "Step 1720. time since epoch: 1510.821. Train acc: 0.747. Train Loss: 0.901\n",
      "Step 1730. time since epoch: 1519.557. Train acc: 0.747. Train Loss: 0.899\n",
      "Step 1740. time since epoch: 1528.370. Train acc: 0.747. Train Loss: 0.898\n",
      "Step 1750. time since epoch: 1537.099. Train acc: 0.748. Train Loss: 0.896\n",
      "Step 1760. time since epoch: 1545.833. Train acc: 0.748. Train Loss: 0.895\n",
      "Step 1770. time since epoch: 1554.557. Train acc: 0.748. Train Loss: 0.894\n",
      "Step 1780. time since epoch: 1563.415. Train acc: 0.748. Train Loss: 0.893\n",
      "Step 1790. time since epoch: 1572.224. Train acc: 0.748. Train Loss: 0.892\n",
      "Step 1800. time since epoch: 1581.049. Train acc: 0.749. Train Loss: 0.890\n",
      "Step 1810. time since epoch: 1589.819. Train acc: 0.749. Train Loss: 0.889\n",
      "Step 1820. time since epoch: 1598.662. Train acc: 0.749. Train Loss: 0.888\n",
      "Step 1830. time since epoch: 1607.562. Train acc: 0.749. Train Loss: 0.887\n",
      "Step 1840. time since epoch: 1616.362. Train acc: 0.749. Train Loss: 0.886\n",
      "Step 1850. time since epoch: 1625.170. Train acc: 0.750. Train Loss: 0.884\n",
      "Step 1860. time since epoch: 1634.000. Train acc: 0.750. Train Loss: 0.883\n",
      "Step 1870. time since epoch: 1642.782. Train acc: 0.750. Train Loss: 0.882\n",
      "Step 1880. time since epoch: 1651.517. Train acc: 0.750. Train Loss: 0.881\n",
      "Step 1890. time since epoch: 1660.324. Train acc: 0.751. Train Loss: 0.880\n",
      "Step 1900. time since epoch: 1669.139. Train acc: 0.751. Train Loss: 0.878\n",
      "Step 1910. time since epoch: 1677.835. Train acc: 0.751. Train Loss: 0.877\n",
      "Step 1920. time since epoch: 1686.560. Train acc: 0.751. Train Loss: 0.876\n",
      "Step 1930. time since epoch: 1695.296. Train acc: 0.751. Train Loss: 0.875\n",
      "Step 1940. time since epoch: 1704.049. Train acc: 0.752. Train Loss: 0.874\n",
      "Step 1950. time since epoch: 1712.794. Train acc: 0.752. Train Loss: 0.873\n",
      "Step 1960. time since epoch: 1721.509. Train acc: 0.752. Train Loss: 0.872\n",
      "Step 1970. time since epoch: 1730.247. Train acc: 0.752. Train Loss: 0.871\n",
      "Step 1980. time since epoch: 1739.050. Train acc: 0.753. Train Loss: 0.870\n",
      "Step 1990. time since epoch: 1747.840. Train acc: 0.753. Train Loss: 0.869\n",
      "Step 2000. time since epoch: 1756.562. Train acc: 0.753. Train Loss: 0.868\n",
      "Step 2010. time since epoch: 1765.304. Train acc: 0.753. Train Loss: 0.866\n",
      "Step 2020. time since epoch: 1774.072. Train acc: 0.753. Train Loss: 0.865\n",
      "Step 2030. time since epoch: 1782.787. Train acc: 0.754. Train Loss: 0.864\n",
      "Step 2040. time since epoch: 1791.536. Train acc: 0.754. Train Loss: 0.863\n",
      "Step 2050. time since epoch: 1800.290. Train acc: 0.754. Train Loss: 0.862\n",
      "Step 2060. time since epoch: 1809.013. Train acc: 0.754. Train Loss: 0.861\n",
      "Step 2070. time since epoch: 1817.720. Train acc: 0.755. Train Loss: 0.860\n",
      "Step 2080. time since epoch: 1826.477. Train acc: 0.755. Train Loss: 0.859\n",
      "Step 2090. time since epoch: 1835.240. Train acc: 0.755. Train Loss: 0.858\n",
      "Step 2100. time since epoch: 1843.965. Train acc: 0.755. Train Loss: 0.857\n",
      "Step 2110. time since epoch: 1852.693. Train acc: 0.755. Train Loss: 0.856\n",
      "Step 2120. time since epoch: 1861.398. Train acc: 0.756. Train Loss: 0.855\n",
      "Step 2130. time since epoch: 1870.065. Train acc: 0.756. Train Loss: 0.854\n",
      "Step 2140. time since epoch: 1878.790. Train acc: 0.756. Train Loss: 0.853\n",
      "Step 2150. time since epoch: 1887.426. Train acc: 0.756. Train Loss: 0.852\n",
      "Step 2160. time since epoch: 1896.092. Train acc: 0.756. Train Loss: 0.851\n",
      "Step 2170. time since epoch: 1904.760. Train acc: 0.757. Train Loss: 0.850\n",
      "Step 2180. time since epoch: 1913.501. Train acc: 0.757. Train Loss: 0.849\n",
      "Step 2190. time since epoch: 1922.269. Train acc: 0.757. Train Loss: 0.848\n",
      "Step 2200. time since epoch: 1931.002. Train acc: 0.757. Train Loss: 0.847\n",
      "Step 2210. time since epoch: 1939.798. Train acc: 0.757. Train Loss: 0.846\n",
      "Step 2220. time since epoch: 1948.622. Train acc: 0.757. Train Loss: 0.845\n",
      "Step 2230. time since epoch: 1957.378. Train acc: 0.758. Train Loss: 0.844\n",
      "Step 2240. time since epoch: 1966.181. Train acc: 0.758. Train Loss: 0.843\n",
      "Step 2250. time since epoch: 1974.953. Train acc: 0.758. Train Loss: 0.842\n",
      "Step 2260. time since epoch: 1983.744. Train acc: 0.758. Train Loss: 0.842\n",
      "Step 2270. time since epoch: 1992.495. Train acc: 0.758. Train Loss: 0.841\n",
      "Step 2280. time since epoch: 2001.352. Train acc: 0.758. Train Loss: 0.840\n",
      "Step 2290. time since epoch: 2010.215. Train acc: 0.759. Train Loss: 0.839\n",
      "Step 2300. time since epoch: 2018.979. Train acc: 0.759. Train Loss: 0.838\n",
      "Step 2310. time since epoch: 2027.776. Train acc: 0.759. Train Loss: 0.837\n",
      "Step 2320. time since epoch: 2036.528. Train acc: 0.759. Train Loss: 0.836\n",
      "Step 2330. time since epoch: 2045.298. Train acc: 0.759. Train Loss: 0.835\n",
      "Step 2340. time since epoch: 2054.104. Train acc: 0.760. Train Loss: 0.834\n",
      "Step 2350. time since epoch: 2062.870. Train acc: 0.760. Train Loss: 0.834\n",
      "Step 2360. time since epoch: 2071.643. Train acc: 0.760. Train Loss: 0.833\n",
      "Step 2370. time since epoch: 2080.426. Train acc: 0.760. Train Loss: 0.832\n",
      "Step 2380. time since epoch: 2089.162. Train acc: 0.760. Train Loss: 0.831\n",
      "Step 2390. time since epoch: 2097.888. Train acc: 0.760. Train Loss: 0.830\n",
      "Step 2400. time since epoch: 2106.564. Train acc: 0.761. Train Loss: 0.829\n",
      "Step 2410. time since epoch: 2115.303. Train acc: 0.761. Train Loss: 0.829\n",
      "Step 2420. time since epoch: 2123.998. Train acc: 0.761. Train Loss: 0.828\n",
      "Step 2430. time since epoch: 2132.751. Train acc: 0.761. Train Loss: 0.827\n",
      "Step 2440. time since epoch: 2141.448. Train acc: 0.761. Train Loss: 0.826\n",
      "Step 2450. time since epoch: 2150.165. Train acc: 0.761. Train Loss: 0.825\n",
      "Step 2460. time since epoch: 2158.869. Train acc: 0.762. Train Loss: 0.824\n",
      "Step 2470. time since epoch: 2167.605. Train acc: 0.762. Train Loss: 0.824\n",
      "Step 2480. time since epoch: 2176.350. Train acc: 0.762. Train Loss: 0.823\n",
      "Step 2490. time since epoch: 2185.095. Train acc: 0.762. Train Loss: 0.822\n",
      "Step 2500. time since epoch: 2193.873. Train acc: 0.762. Train Loss: 0.821\n",
      "Step 2510. time since epoch: 2202.643. Train acc: 0.762. Train Loss: 0.821\n",
      "Step 2520. time since epoch: 2211.382. Train acc: 0.762. Train Loss: 0.820\n",
      "Step 2530. time since epoch: 2220.165. Train acc: 0.763. Train Loss: 0.819\n",
      "Step 2540. time since epoch: 2228.912. Train acc: 0.763. Train Loss: 0.818\n",
      "Step 2550. time since epoch: 2237.637. Train acc: 0.763. Train Loss: 0.817\n",
      "Step 2560. time since epoch: 2246.389. Train acc: 0.763. Train Loss: 0.817\n",
      "Step 2570. time since epoch: 2255.084. Train acc: 0.763. Train Loss: 0.816\n",
      "Step 2580. time since epoch: 2263.802. Train acc: 0.763. Train Loss: 0.815\n",
      "Step 2590. time since epoch: 2272.636. Train acc: 0.763. Train Loss: 0.814\n",
      "Step 2600. time since epoch: 2281.370. Train acc: 0.764. Train Loss: 0.814\n",
      "Step 2610. time since epoch: 2290.152. Train acc: 0.764. Train Loss: 0.813\n",
      "Step 2620. time since epoch: 2298.899. Train acc: 0.764. Train Loss: 0.812\n",
      "Step 2630. time since epoch: 2307.573. Train acc: 0.764. Train Loss: 0.812\n",
      "Step 2640. time since epoch: 2316.312. Train acc: 0.764. Train Loss: 0.811\n",
      "Step 2650. time since epoch: 2325.081. Train acc: 0.764. Train Loss: 0.810\n",
      "Step 2660. time since epoch: 2333.802. Train acc: 0.765. Train Loss: 0.809\n",
      "Step 2670. time since epoch: 2342.514. Train acc: 0.765. Train Loss: 0.809\n",
      "Step 2680. time since epoch: 2351.247. Train acc: 0.765. Train Loss: 0.808\n",
      "Step 2690. time since epoch: 2360.035. Train acc: 0.765. Train Loss: 0.808\n",
      "Step 2700. time since epoch: 2368.809. Train acc: 0.765. Train Loss: 0.807\n",
      "Step 2710. time since epoch: 2377.629. Train acc: 0.765. Train Loss: 0.806\n",
      "Step 2720. time since epoch: 2386.485. Train acc: 0.765. Train Loss: 0.805\n",
      "--------------------\n",
      "epoch 1, loss 0.8050, train acc 0.765, test acc 0.806, time 2671.9 sec\n"
     ]
    }
   ],
   "source": [
    "train(model, train_iter, test_iter, trainer, 1, list_of_losses=dense_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kosar\\anaconda3\\envs\\mult_nn_net\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = tv.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(in_features=512, out_features=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.Adam(params_to_update, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0. time since epoch: 0.281. Train acc: 0.020. Train Loss: 4.394\n",
      "Step 10. time since epoch: 3.006. Train acc: 0.078. Train Loss: 3.758\n",
      "Step 20. time since epoch: 5.689. Train acc: 0.180. Train Loss: 3.463\n",
      "Step 30. time since epoch: 8.418. Train acc: 0.247. Train Loss: 3.256\n",
      "Step 40. time since epoch: 11.152. Train acc: 0.294. Train Loss: 3.077\n",
      "Step 50. time since epoch: 13.883. Train acc: 0.334. Train Loss: 2.924\n",
      "Step 60. time since epoch: 16.510. Train acc: 0.370. Train Loss: 2.786\n",
      "Step 70. time since epoch: 19.117. Train acc: 0.398. Train Loss: 2.663\n",
      "Step 80. time since epoch: 21.804. Train acc: 0.423. Train Loss: 2.556\n",
      "Step 90. time since epoch: 24.504. Train acc: 0.445. Train Loss: 2.461\n",
      "Step 100. time since epoch: 27.287. Train acc: 0.462. Train Loss: 2.379\n",
      "Step 110. time since epoch: 30.010. Train acc: 0.477. Train Loss: 2.308\n",
      "Step 120. time since epoch: 32.694. Train acc: 0.490. Train Loss: 2.239\n",
      "Step 130. time since epoch: 35.428. Train acc: 0.503. Train Loss: 2.176\n",
      "Step 140. time since epoch: 38.156. Train acc: 0.514. Train Loss: 2.123\n",
      "Step 150. time since epoch: 40.884. Train acc: 0.524. Train Loss: 2.074\n",
      "Step 160. time since epoch: 43.653. Train acc: 0.533. Train Loss: 2.026\n",
      "Step 170. time since epoch: 46.364. Train acc: 0.543. Train Loss: 1.979\n",
      "Step 180. time since epoch: 49.267. Train acc: 0.552. Train Loss: 1.938\n",
      "Step 190. time since epoch: 51.963. Train acc: 0.559. Train Loss: 1.901\n",
      "Step 200. time since epoch: 54.726. Train acc: 0.567. Train Loss: 1.864\n",
      "Step 210. time since epoch: 57.393. Train acc: 0.573. Train Loss: 1.831\n",
      "Step 220. time since epoch: 60.130. Train acc: 0.579. Train Loss: 1.801\n",
      "Step 230. time since epoch: 62.916. Train acc: 0.584. Train Loss: 1.774\n",
      "Step 240. time since epoch: 65.566. Train acc: 0.589. Train Loss: 1.748\n",
      "Step 250. time since epoch: 68.276. Train acc: 0.594. Train Loss: 1.722\n",
      "Step 260. time since epoch: 70.987. Train acc: 0.599. Train Loss: 1.698\n",
      "Step 270. time since epoch: 73.744. Train acc: 0.603. Train Loss: 1.675\n",
      "Step 280. time since epoch: 76.494. Train acc: 0.608. Train Loss: 1.652\n",
      "Step 290. time since epoch: 79.167. Train acc: 0.612. Train Loss: 1.631\n",
      "Step 300. time since epoch: 81.849. Train acc: 0.615. Train Loss: 1.612\n",
      "Step 310. time since epoch: 84.548. Train acc: 0.619. Train Loss: 1.593\n",
      "Step 320. time since epoch: 87.188. Train acc: 0.623. Train Loss: 1.574\n",
      "Step 330. time since epoch: 89.861. Train acc: 0.626. Train Loss: 1.555\n",
      "Step 340. time since epoch: 92.608. Train acc: 0.629. Train Loss: 1.539\n",
      "Step 350. time since epoch: 95.267. Train acc: 0.633. Train Loss: 1.523\n",
      "Step 360. time since epoch: 97.954. Train acc: 0.635. Train Loss: 1.508\n",
      "Step 370. time since epoch: 100.596. Train acc: 0.638. Train Loss: 1.494\n",
      "Step 380. time since epoch: 103.340. Train acc: 0.640. Train Loss: 1.480\n",
      "Step 390. time since epoch: 106.009. Train acc: 0.643. Train Loss: 1.466\n",
      "Step 400. time since epoch: 108.651. Train acc: 0.645. Train Loss: 1.453\n",
      "Step 410. time since epoch: 111.239. Train acc: 0.648. Train Loss: 1.440\n",
      "Step 420. time since epoch: 113.983. Train acc: 0.650. Train Loss: 1.428\n",
      "Step 430. time since epoch: 116.685. Train acc: 0.652. Train Loss: 1.416\n",
      "Step 440. time since epoch: 119.330. Train acc: 0.654. Train Loss: 1.405\n",
      "Step 450. time since epoch: 122.049. Train acc: 0.656. Train Loss: 1.395\n",
      "Step 460. time since epoch: 124.674. Train acc: 0.658. Train Loss: 1.384\n",
      "Step 470. time since epoch: 127.333. Train acc: 0.660. Train Loss: 1.374\n",
      "Step 480. time since epoch: 130.011. Train acc: 0.662. Train Loss: 1.363\n",
      "Step 490. time since epoch: 132.773. Train acc: 0.664. Train Loss: 1.353\n",
      "Step 500. time since epoch: 135.387. Train acc: 0.665. Train Loss: 1.344\n",
      "Step 510. time since epoch: 138.073. Train acc: 0.667. Train Loss: 1.334\n",
      "Step 520. time since epoch: 140.792. Train acc: 0.669. Train Loss: 1.326\n",
      "Step 530. time since epoch: 143.424. Train acc: 0.670. Train Loss: 1.317\n",
      "Step 540. time since epoch: 146.041. Train acc: 0.672. Train Loss: 1.308\n",
      "Step 550. time since epoch: 148.765. Train acc: 0.674. Train Loss: 1.299\n",
      "Step 560. time since epoch: 151.437. Train acc: 0.676. Train Loss: 1.291\n",
      "Step 570. time since epoch: 154.133. Train acc: 0.677. Train Loss: 1.283\n",
      "Step 580. time since epoch: 156.786. Train acc: 0.678. Train Loss: 1.276\n",
      "Step 590. time since epoch: 159.449. Train acc: 0.680. Train Loss: 1.268\n",
      "Step 600. time since epoch: 162.173. Train acc: 0.681. Train Loss: 1.260\n",
      "Step 610. time since epoch: 164.871. Train acc: 0.682. Train Loss: 1.253\n",
      "Step 620. time since epoch: 167.482. Train acc: 0.684. Train Loss: 1.246\n",
      "Step 630. time since epoch: 170.251. Train acc: 0.685. Train Loss: 1.239\n",
      "Step 640. time since epoch: 173.003. Train acc: 0.686. Train Loss: 1.232\n",
      "Step 650. time since epoch: 175.755. Train acc: 0.688. Train Loss: 1.225\n",
      "Step 660. time since epoch: 178.388. Train acc: 0.689. Train Loss: 1.219\n",
      "Step 670. time since epoch: 181.049. Train acc: 0.690. Train Loss: 1.213\n",
      "Step 680. time since epoch: 183.662. Train acc: 0.691. Train Loss: 1.207\n",
      "Step 690. time since epoch: 186.326. Train acc: 0.692. Train Loss: 1.201\n",
      "Step 700. time since epoch: 189.002. Train acc: 0.693. Train Loss: 1.196\n",
      "Step 710. time since epoch: 191.736. Train acc: 0.694. Train Loss: 1.190\n",
      "Step 720. time since epoch: 194.427. Train acc: 0.695. Train Loss: 1.185\n",
      "Step 730. time since epoch: 197.040. Train acc: 0.696. Train Loss: 1.180\n",
      "Step 740. time since epoch: 199.707. Train acc: 0.697. Train Loss: 1.175\n",
      "Step 750. time since epoch: 202.458. Train acc: 0.698. Train Loss: 1.169\n",
      "Step 760. time since epoch: 205.099. Train acc: 0.699. Train Loss: 1.165\n",
      "Step 770. time since epoch: 207.790. Train acc: 0.700. Train Loss: 1.159\n",
      "Step 780. time since epoch: 210.443. Train acc: 0.701. Train Loss: 1.154\n",
      "Step 790. time since epoch: 213.039. Train acc: 0.702. Train Loss: 1.149\n",
      "Step 800. time since epoch: 215.705. Train acc: 0.703. Train Loss: 1.145\n",
      "Step 810. time since epoch: 218.260. Train acc: 0.703. Train Loss: 1.140\n",
      "Step 820. time since epoch: 220.851. Train acc: 0.704. Train Loss: 1.135\n",
      "Step 830. time since epoch: 223.513. Train acc: 0.705. Train Loss: 1.131\n",
      "Step 840. time since epoch: 226.225. Train acc: 0.706. Train Loss: 1.127\n",
      "Step 850. time since epoch: 228.835. Train acc: 0.707. Train Loss: 1.122\n",
      "Step 860. time since epoch: 231.741. Train acc: 0.708. Train Loss: 1.118\n",
      "Step 870. time since epoch: 234.420. Train acc: 0.708. Train Loss: 1.114\n",
      "Step 880. time since epoch: 237.191. Train acc: 0.709. Train Loss: 1.110\n",
      "Step 890. time since epoch: 239.822. Train acc: 0.710. Train Loss: 1.106\n",
      "Step 900. time since epoch: 242.446. Train acc: 0.710. Train Loss: 1.102\n",
      "Step 910. time since epoch: 245.060. Train acc: 0.711. Train Loss: 1.098\n",
      "Step 920. time since epoch: 247.651. Train acc: 0.712. Train Loss: 1.095\n",
      "Step 930. time since epoch: 250.252. Train acc: 0.712. Train Loss: 1.092\n",
      "Step 940. time since epoch: 252.884. Train acc: 0.713. Train Loss: 1.088\n",
      "Step 950. time since epoch: 255.459. Train acc: 0.714. Train Loss: 1.084\n",
      "Step 960. time since epoch: 258.166. Train acc: 0.714. Train Loss: 1.081\n",
      "Step 970. time since epoch: 260.845. Train acc: 0.715. Train Loss: 1.077\n",
      "Step 980. time since epoch: 263.518. Train acc: 0.716. Train Loss: 1.074\n",
      "Step 990. time since epoch: 266.185. Train acc: 0.716. Train Loss: 1.071\n",
      "Step 1000. time since epoch: 268.850. Train acc: 0.717. Train Loss: 1.067\n",
      "Step 1010. time since epoch: 271.500. Train acc: 0.717. Train Loss: 1.064\n",
      "Step 1020. time since epoch: 274.111. Train acc: 0.718. Train Loss: 1.061\n",
      "Step 1030. time since epoch: 276.742. Train acc: 0.719. Train Loss: 1.057\n",
      "Step 1040. time since epoch: 279.307. Train acc: 0.719. Train Loss: 1.054\n",
      "Step 1050. time since epoch: 281.963. Train acc: 0.720. Train Loss: 1.051\n",
      "Step 1060. time since epoch: 284.511. Train acc: 0.721. Train Loss: 1.048\n",
      "Step 1070. time since epoch: 287.127. Train acc: 0.721. Train Loss: 1.045\n",
      "Step 1080. time since epoch: 289.748. Train acc: 0.721. Train Loss: 1.043\n",
      "Step 1090. time since epoch: 292.330. Train acc: 0.722. Train Loss: 1.039\n",
      "Step 1100. time since epoch: 295.009. Train acc: 0.723. Train Loss: 1.037\n",
      "Step 1110. time since epoch: 297.638. Train acc: 0.723. Train Loss: 1.034\n",
      "Step 1120. time since epoch: 300.272. Train acc: 0.724. Train Loss: 1.031\n",
      "Step 1130. time since epoch: 302.816. Train acc: 0.724. Train Loss: 1.029\n",
      "Step 1140. time since epoch: 305.444. Train acc: 0.724. Train Loss: 1.026\n",
      "Step 1150. time since epoch: 308.116. Train acc: 0.725. Train Loss: 1.023\n",
      "Step 1160. time since epoch: 310.796. Train acc: 0.725. Train Loss: 1.021\n",
      "Step 1170. time since epoch: 313.668. Train acc: 0.726. Train Loss: 1.018\n",
      "Step 1180. time since epoch: 316.372. Train acc: 0.726. Train Loss: 1.015\n",
      "Step 1190. time since epoch: 319.065. Train acc: 0.727. Train Loss: 1.013\n",
      "Step 1200. time since epoch: 321.788. Train acc: 0.727. Train Loss: 1.010\n",
      "Step 1210. time since epoch: 324.491. Train acc: 0.728. Train Loss: 1.008\n",
      "Step 1220. time since epoch: 327.240. Train acc: 0.728. Train Loss: 1.006\n",
      "Step 1230. time since epoch: 329.908. Train acc: 0.729. Train Loss: 1.003\n",
      "Step 1240. time since epoch: 332.651. Train acc: 0.729. Train Loss: 1.000\n",
      "Step 1250. time since epoch: 335.400. Train acc: 0.730. Train Loss: 0.998\n",
      "Step 1260. time since epoch: 338.075. Train acc: 0.730. Train Loss: 0.996\n",
      "Step 1270. time since epoch: 340.812. Train acc: 0.730. Train Loss: 0.993\n",
      "Step 1280. time since epoch: 343.446. Train acc: 0.731. Train Loss: 0.991\n",
      "Step 1290. time since epoch: 346.126. Train acc: 0.731. Train Loss: 0.989\n",
      "Step 1300. time since epoch: 348.743. Train acc: 0.732. Train Loss: 0.987\n",
      "Step 1310. time since epoch: 351.370. Train acc: 0.732. Train Loss: 0.985\n",
      "Step 1320. time since epoch: 354.063. Train acc: 0.732. Train Loss: 0.983\n",
      "Step 1330. time since epoch: 356.726. Train acc: 0.733. Train Loss: 0.980\n",
      "Step 1340. time since epoch: 359.358. Train acc: 0.733. Train Loss: 0.978\n",
      "Step 1350. time since epoch: 362.001. Train acc: 0.734. Train Loss: 0.976\n",
      "Step 1360. time since epoch: 364.715. Train acc: 0.734. Train Loss: 0.974\n",
      "Step 1370. time since epoch: 367.432. Train acc: 0.734. Train Loss: 0.972\n",
      "Step 1380. time since epoch: 370.099. Train acc: 0.735. Train Loss: 0.970\n",
      "Step 1390. time since epoch: 373.009. Train acc: 0.735. Train Loss: 0.968\n",
      "Step 1400. time since epoch: 375.689. Train acc: 0.735. Train Loss: 0.966\n",
      "Step 1410. time since epoch: 378.434. Train acc: 0.736. Train Loss: 0.965\n",
      "Step 1420. time since epoch: 381.144. Train acc: 0.736. Train Loss: 0.963\n",
      "Step 1430. time since epoch: 383.875. Train acc: 0.736. Train Loss: 0.961\n",
      "Step 1440. time since epoch: 386.616. Train acc: 0.737. Train Loss: 0.959\n",
      "Step 1450. time since epoch: 389.295. Train acc: 0.737. Train Loss: 0.957\n",
      "Step 1460. time since epoch: 391.953. Train acc: 0.737. Train Loss: 0.955\n",
      "Step 1470. time since epoch: 394.641. Train acc: 0.738. Train Loss: 0.954\n",
      "Step 1480. time since epoch: 397.332. Train acc: 0.738. Train Loss: 0.952\n",
      "Step 1490. time since epoch: 400.011. Train acc: 0.738. Train Loss: 0.950\n",
      "Step 1500. time since epoch: 402.723. Train acc: 0.739. Train Loss: 0.949\n",
      "Step 1510. time since epoch: 405.467. Train acc: 0.739. Train Loss: 0.947\n",
      "Step 1520. time since epoch: 408.140. Train acc: 0.739. Train Loss: 0.945\n",
      "Step 1530. time since epoch: 411.006. Train acc: 0.740. Train Loss: 0.943\n",
      "Step 1540. time since epoch: 413.801. Train acc: 0.740. Train Loss: 0.942\n",
      "Step 1550. time since epoch: 416.477. Train acc: 0.740. Train Loss: 0.940\n",
      "Step 1560. time since epoch: 419.148. Train acc: 0.741. Train Loss: 0.938\n",
      "Step 1570. time since epoch: 421.799. Train acc: 0.741. Train Loss: 0.937\n",
      "Step 1580. time since epoch: 424.473. Train acc: 0.741. Train Loss: 0.935\n",
      "Step 1590. time since epoch: 427.155. Train acc: 0.741. Train Loss: 0.934\n",
      "Step 1600. time since epoch: 429.877. Train acc: 0.742. Train Loss: 0.932\n",
      "Step 1610. time since epoch: 432.749. Train acc: 0.742. Train Loss: 0.931\n",
      "Step 1620. time since epoch: 435.501. Train acc: 0.742. Train Loss: 0.929\n",
      "Step 1630. time since epoch: 438.183. Train acc: 0.743. Train Loss: 0.927\n",
      "Step 1640. time since epoch: 440.882. Train acc: 0.743. Train Loss: 0.926\n",
      "Step 1650. time since epoch: 443.579. Train acc: 0.743. Train Loss: 0.924\n",
      "Step 1660. time since epoch: 446.305. Train acc: 0.744. Train Loss: 0.923\n",
      "Step 1670. time since epoch: 449.017. Train acc: 0.744. Train Loss: 0.921\n",
      "Step 1680. time since epoch: 451.771. Train acc: 0.744. Train Loss: 0.920\n",
      "Step 1690. time since epoch: 454.446. Train acc: 0.744. Train Loss: 0.918\n",
      "Step 1700. time since epoch: 457.164. Train acc: 0.745. Train Loss: 0.917\n",
      "Step 1710. time since epoch: 459.829. Train acc: 0.745. Train Loss: 0.916\n",
      "Step 1720. time since epoch: 462.438. Train acc: 0.745. Train Loss: 0.914\n",
      "Step 1730. time since epoch: 465.146. Train acc: 0.745. Train Loss: 0.913\n",
      "Step 1740. time since epoch: 467.843. Train acc: 0.746. Train Loss: 0.911\n",
      "Step 1750. time since epoch: 470.502. Train acc: 0.746. Train Loss: 0.910\n",
      "Step 1760. time since epoch: 473.170. Train acc: 0.746. Train Loss: 0.909\n",
      "Step 1770. time since epoch: 475.930. Train acc: 0.746. Train Loss: 0.907\n",
      "Step 1780. time since epoch: 478.622. Train acc: 0.747. Train Loss: 0.906\n",
      "Step 1790. time since epoch: 481.386. Train acc: 0.747. Train Loss: 0.905\n",
      "Step 1800. time since epoch: 484.155. Train acc: 0.747. Train Loss: 0.904\n",
      "Step 1810. time since epoch: 486.904. Train acc: 0.747. Train Loss: 0.903\n",
      "Step 1820. time since epoch: 489.653. Train acc: 0.747. Train Loss: 0.902\n",
      "Step 1830. time since epoch: 492.475. Train acc: 0.748. Train Loss: 0.900\n",
      "Step 1840. time since epoch: 495.191. Train acc: 0.748. Train Loss: 0.899\n",
      "Step 1850. time since epoch: 497.902. Train acc: 0.748. Train Loss: 0.898\n",
      "Step 1860. time since epoch: 500.551. Train acc: 0.748. Train Loss: 0.896\n",
      "Step 1870. time since epoch: 503.320. Train acc: 0.748. Train Loss: 0.895\n",
      "Step 1880. time since epoch: 506.086. Train acc: 0.749. Train Loss: 0.894\n",
      "Step 1890. time since epoch: 508.745. Train acc: 0.749. Train Loss: 0.893\n",
      "Step 1900. time since epoch: 511.551. Train acc: 0.749. Train Loss: 0.892\n",
      "Step 1910. time since epoch: 514.215. Train acc: 0.749. Train Loss: 0.890\n",
      "Step 1920. time since epoch: 516.884. Train acc: 0.750. Train Loss: 0.889\n",
      "Step 1930. time since epoch: 519.601. Train acc: 0.750. Train Loss: 0.888\n",
      "Step 1940. time since epoch: 522.360. Train acc: 0.750. Train Loss: 0.887\n",
      "Step 1950. time since epoch: 525.105. Train acc: 0.750. Train Loss: 0.886\n",
      "Step 1960. time since epoch: 527.837. Train acc: 0.750. Train Loss: 0.885\n",
      "Step 1970. time since epoch: 530.593. Train acc: 0.751. Train Loss: 0.884\n",
      "Step 1980. time since epoch: 533.330. Train acc: 0.751. Train Loss: 0.883\n",
      "Step 1990. time since epoch: 536.099. Train acc: 0.751. Train Loss: 0.882\n",
      "Step 2000. time since epoch: 538.767. Train acc: 0.751. Train Loss: 0.880\n",
      "Step 2010. time since epoch: 541.476. Train acc: 0.752. Train Loss: 0.879\n",
      "Step 2020. time since epoch: 544.103. Train acc: 0.752. Train Loss: 0.878\n",
      "Step 2030. time since epoch: 546.776. Train acc: 0.752. Train Loss: 0.877\n",
      "Step 2040. time since epoch: 549.466. Train acc: 0.752. Train Loss: 0.876\n",
      "Step 2050. time since epoch: 552.205. Train acc: 0.752. Train Loss: 0.875\n",
      "Step 2060. time since epoch: 554.867. Train acc: 0.753. Train Loss: 0.874\n",
      "Step 2070. time since epoch: 557.534. Train acc: 0.753. Train Loss: 0.873\n",
      "Step 2080. time since epoch: 560.204. Train acc: 0.753. Train Loss: 0.872\n",
      "Step 2090. time since epoch: 562.885. Train acc: 0.753. Train Loss: 0.871\n",
      "Step 2100. time since epoch: 565.515. Train acc: 0.753. Train Loss: 0.870\n",
      "Step 2110. time since epoch: 568.112. Train acc: 0.754. Train Loss: 0.869\n",
      "Step 2120. time since epoch: 570.777. Train acc: 0.754. Train Loss: 0.868\n",
      "Step 2130. time since epoch: 573.497. Train acc: 0.754. Train Loss: 0.867\n",
      "Step 2140. time since epoch: 576.226. Train acc: 0.754. Train Loss: 0.865\n",
      "Step 2150. time since epoch: 578.851. Train acc: 0.754. Train Loss: 0.864\n",
      "Step 2160. time since epoch: 581.560. Train acc: 0.755. Train Loss: 0.863\n",
      "Step 2170. time since epoch: 584.287. Train acc: 0.755. Train Loss: 0.862\n",
      "Step 2180. time since epoch: 586.973. Train acc: 0.755. Train Loss: 0.861\n",
      "Step 2190. time since epoch: 589.580. Train acc: 0.755. Train Loss: 0.860\n",
      "Step 2200. time since epoch: 592.189. Train acc: 0.755. Train Loss: 0.860\n",
      "Step 2210. time since epoch: 594.850. Train acc: 0.755. Train Loss: 0.859\n",
      "Step 2220. time since epoch: 597.590. Train acc: 0.756. Train Loss: 0.858\n",
      "Step 2230. time since epoch: 600.289. Train acc: 0.756. Train Loss: 0.857\n",
      "Step 2240. time since epoch: 602.975. Train acc: 0.756. Train Loss: 0.856\n",
      "Step 2250. time since epoch: 605.684. Train acc: 0.756. Train Loss: 0.855\n",
      "Step 2260. time since epoch: 608.361. Train acc: 0.756. Train Loss: 0.854\n",
      "Step 2270. time since epoch: 611.022. Train acc: 0.756. Train Loss: 0.854\n",
      "Step 2280. time since epoch: 613.668. Train acc: 0.756. Train Loss: 0.853\n",
      "Step 2290. time since epoch: 616.375. Train acc: 0.757. Train Loss: 0.852\n",
      "Step 2300. time since epoch: 619.078. Train acc: 0.757. Train Loss: 0.851\n",
      "Step 2310. time since epoch: 621.778. Train acc: 0.757. Train Loss: 0.850\n",
      "Step 2320. time since epoch: 624.462. Train acc: 0.757. Train Loss: 0.849\n",
      "Step 2330. time since epoch: 627.108. Train acc: 0.757. Train Loss: 0.848\n",
      "Step 2340. time since epoch: 629.820. Train acc: 0.757. Train Loss: 0.847\n",
      "Step 2350. time since epoch: 632.521. Train acc: 0.758. Train Loss: 0.846\n",
      "Step 2360. time since epoch: 635.165. Train acc: 0.758. Train Loss: 0.845\n",
      "Step 2370. time since epoch: 637.800. Train acc: 0.758. Train Loss: 0.845\n",
      "Step 2380. time since epoch: 640.581. Train acc: 0.758. Train Loss: 0.844\n",
      "Step 2390. time since epoch: 643.320. Train acc: 0.758. Train Loss: 0.843\n",
      "Step 2400. time since epoch: 645.949. Train acc: 0.758. Train Loss: 0.842\n",
      "Step 2410. time since epoch: 648.568. Train acc: 0.759. Train Loss: 0.841\n",
      "Step 2420. time since epoch: 651.439. Train acc: 0.759. Train Loss: 0.841\n",
      "Step 2430. time since epoch: 654.098. Train acc: 0.759. Train Loss: 0.840\n",
      "Step 2440. time since epoch: 656.707. Train acc: 0.759. Train Loss: 0.839\n",
      "Step 2450. time since epoch: 659.359. Train acc: 0.759. Train Loss: 0.838\n",
      "Step 2460. time since epoch: 661.970. Train acc: 0.760. Train Loss: 0.837\n",
      "Step 2470. time since epoch: 664.639. Train acc: 0.760. Train Loss: 0.836\n",
      "Step 2480. time since epoch: 667.370. Train acc: 0.760. Train Loss: 0.836\n",
      "Step 2490. time since epoch: 670.009. Train acc: 0.760. Train Loss: 0.835\n",
      "Step 2500. time since epoch: 672.669. Train acc: 0.760. Train Loss: 0.834\n",
      "Step 2510. time since epoch: 675.418. Train acc: 0.760. Train Loss: 0.833\n",
      "Step 2520. time since epoch: 678.245. Train acc: 0.760. Train Loss: 0.832\n",
      "Step 2530. time since epoch: 680.915. Train acc: 0.761. Train Loss: 0.832\n",
      "Step 2540. time since epoch: 683.654. Train acc: 0.761. Train Loss: 0.831\n",
      "Step 2550. time since epoch: 686.342. Train acc: 0.761. Train Loss: 0.830\n",
      "Step 2560. time since epoch: 689.102. Train acc: 0.761. Train Loss: 0.829\n",
      "Step 2570. time since epoch: 691.885. Train acc: 0.761. Train Loss: 0.828\n",
      "Step 2580. time since epoch: 694.585. Train acc: 0.761. Train Loss: 0.828\n",
      "Step 2590. time since epoch: 697.307. Train acc: 0.761. Train Loss: 0.827\n",
      "Step 2600. time since epoch: 699.940. Train acc: 0.761. Train Loss: 0.826\n",
      "Step 2610. time since epoch: 702.669. Train acc: 0.762. Train Loss: 0.826\n",
      "Step 2620. time since epoch: 705.431. Train acc: 0.762. Train Loss: 0.825\n",
      "Step 2630. time since epoch: 708.164. Train acc: 0.762. Train Loss: 0.824\n",
      "Step 2640. time since epoch: 710.834. Train acc: 0.762. Train Loss: 0.823\n",
      "Step 2650. time since epoch: 713.572. Train acc: 0.762. Train Loss: 0.822\n",
      "Step 2660. time since epoch: 716.264. Train acc: 0.762. Train Loss: 0.822\n",
      "Step 2670. time since epoch: 718.989. Train acc: 0.762. Train Loss: 0.821\n",
      "Step 2680. time since epoch: 721.685. Train acc: 0.763. Train Loss: 0.821\n",
      "Step 2690. time since epoch: 724.362. Train acc: 0.763. Train Loss: 0.820\n",
      "Step 2700. time since epoch: 727.124. Train acc: 0.763. Train Loss: 0.819\n",
      "Step 2710. time since epoch: 729.899. Train acc: 0.763. Train Loss: 0.819\n",
      "Step 2720. time since epoch: 732.792. Train acc: 0.763. Train Loss: 0.818\n",
      "--------------------\n",
      "epoch 1, loss 0.8175, train acc 0.763, test acc 0.802, time 814.6 sec\n"
     ]
    }
   ],
   "source": [
    "train(model, train_iter, test_iter, trainer, 1, list_of_losses=res_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kosar\\anaconda3\\envs\\mult_nn_net\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = tv.models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сделаем модель попроще"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(25088, 1024),  \n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(1024, 512),   \n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 62)      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.0.weight\n",
      "\t classifier.0.bias\n",
      "\t classifier.3.weight\n",
      "\t classifier.3.bias\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.Adam(params_to_update, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0. time since epoch: 3.509. Train acc: 0.008. Train Loss: 4.131\n",
      "Step 10. time since epoch: 32.550. Train acc: 0.303. Train Loss: 3.050\n",
      "Step 20. time since epoch: 61.465. Train acc: 0.432. Train Loss: 2.415\n",
      "Step 30. time since epoch: 90.213. Train acc: 0.499. Train Loss: 2.086\n",
      "Step 40. time since epoch: 118.855. Train acc: 0.541. Train Loss: 1.848\n",
      "Step 50. time since epoch: 147.545. Train acc: 0.576. Train Loss: 1.671\n",
      "Step 60. time since epoch: 176.282. Train acc: 0.599. Train Loss: 1.547\n",
      "Step 70. time since epoch: 205.090. Train acc: 0.620. Train Loss: 1.444\n",
      "Step 80. time since epoch: 233.883. Train acc: 0.636. Train Loss: 1.368\n",
      "Step 90. time since epoch: 262.782. Train acc: 0.649. Train Loss: 1.301\n",
      "Step 100. time since epoch: 291.631. Train acc: 0.660. Train Loss: 1.247\n",
      "Step 110. time since epoch: 320.499. Train acc: 0.668. Train Loss: 1.204\n",
      "Step 120. time since epoch: 349.418. Train acc: 0.677. Train Loss: 1.160\n",
      "Step 130. time since epoch: 378.069. Train acc: 0.686. Train Loss: 1.122\n",
      "Step 140. time since epoch: 406.989. Train acc: 0.692. Train Loss: 1.092\n",
      "Step 150. time since epoch: 435.782. Train acc: 0.697. Train Loss: 1.068\n",
      "Step 160. time since epoch: 464.580. Train acc: 0.703. Train Loss: 1.042\n",
      "Step 170. time since epoch: 493.268. Train acc: 0.708. Train Loss: 1.019\n",
      "Step 180. time since epoch: 522.055. Train acc: 0.713. Train Loss: 0.998\n",
      "Step 190. time since epoch: 550.789. Train acc: 0.717. Train Loss: 0.979\n",
      "Step 200. time since epoch: 579.611. Train acc: 0.722. Train Loss: 0.961\n",
      "Step 210. time since epoch: 608.970. Train acc: 0.725. Train Loss: 0.946\n",
      "Step 220. time since epoch: 638.176. Train acc: 0.728. Train Loss: 0.933\n",
      "Step 230. time since epoch: 667.248. Train acc: 0.731. Train Loss: 0.920\n",
      "Step 240. time since epoch: 696.282. Train acc: 0.733. Train Loss: 0.909\n",
      "Step 250. time since epoch: 725.303. Train acc: 0.735. Train Loss: 0.898\n",
      "Step 260. time since epoch: 754.352. Train acc: 0.738. Train Loss: 0.887\n",
      "Step 270. time since epoch: 783.371. Train acc: 0.740. Train Loss: 0.877\n",
      "Step 280. time since epoch: 812.449. Train acc: 0.742. Train Loss: 0.867\n",
      "Step 290. time since epoch: 841.448. Train acc: 0.744. Train Loss: 0.858\n",
      "Step 300. time since epoch: 870.534. Train acc: 0.746. Train Loss: 0.850\n",
      "Step 310. time since epoch: 899.593. Train acc: 0.748. Train Loss: 0.843\n",
      "Step 320. time since epoch: 928.621. Train acc: 0.750. Train Loss: 0.835\n",
      "Step 330. time since epoch: 957.618. Train acc: 0.752. Train Loss: 0.827\n",
      "Step 340. time since epoch: 986.792. Train acc: 0.754. Train Loss: 0.821\n",
      "Step 350. time since epoch: 1015.862. Train acc: 0.755. Train Loss: 0.815\n",
      "Step 360. time since epoch: 1044.933. Train acc: 0.756. Train Loss: 0.809\n",
      "Step 370. time since epoch: 1074.064. Train acc: 0.757. Train Loss: 0.804\n",
      "Step 380. time since epoch: 1103.105. Train acc: 0.758. Train Loss: 0.798\n",
      "Step 390. time since epoch: 1132.182. Train acc: 0.759. Train Loss: 0.792\n",
      "Step 400. time since epoch: 1161.293. Train acc: 0.761. Train Loss: 0.787\n",
      "Step 410. time since epoch: 1190.450. Train acc: 0.761. Train Loss: 0.781\n",
      "Step 420. time since epoch: 1219.506. Train acc: 0.762. Train Loss: 0.777\n",
      "Step 430. time since epoch: 1248.446. Train acc: 0.763. Train Loss: 0.772\n",
      "Step 440. time since epoch: 1277.517. Train acc: 0.764. Train Loss: 0.768\n",
      "Step 450. time since epoch: 1306.575. Train acc: 0.765. Train Loss: 0.764\n",
      "Step 460. time since epoch: 1335.672. Train acc: 0.767. Train Loss: 0.759\n",
      "Step 470. time since epoch: 1364.849. Train acc: 0.767. Train Loss: 0.755\n",
      "Step 480. time since epoch: 1393.966. Train acc: 0.769. Train Loss: 0.750\n",
      "Step 490. time since epoch: 1423.028. Train acc: 0.769. Train Loss: 0.746\n",
      "Step 500. time since epoch: 1452.162. Train acc: 0.770. Train Loss: 0.742\n",
      "Step 510. time since epoch: 1481.058. Train acc: 0.771. Train Loss: 0.739\n",
      "Step 520. time since epoch: 1510.098. Train acc: 0.771. Train Loss: 0.736\n",
      "Step 530. time since epoch: 1539.217. Train acc: 0.772. Train Loss: 0.733\n",
      "Step 540. time since epoch: 1568.299. Train acc: 0.773. Train Loss: 0.729\n",
      "Step 550. time since epoch: 1597.323. Train acc: 0.774. Train Loss: 0.726\n",
      "Step 560. time since epoch: 1626.281. Train acc: 0.774. Train Loss: 0.724\n",
      "Step 570. time since epoch: 1655.302. Train acc: 0.775. Train Loss: 0.721\n",
      "Step 580. time since epoch: 1684.484. Train acc: 0.775. Train Loss: 0.718\n",
      "Step 590. time since epoch: 1713.559. Train acc: 0.776. Train Loss: 0.715\n",
      "Step 600. time since epoch: 1742.650. Train acc: 0.777. Train Loss: 0.713\n",
      "Step 610. time since epoch: 1771.674. Train acc: 0.777. Train Loss: 0.710\n",
      "Step 620. time since epoch: 1800.758. Train acc: 0.778. Train Loss: 0.707\n",
      "Step 630. time since epoch: 1829.833. Train acc: 0.779. Train Loss: 0.705\n",
      "Step 640. time since epoch: 1858.754. Train acc: 0.779. Train Loss: 0.702\n",
      "Step 650. time since epoch: 1887.764. Train acc: 0.780. Train Loss: 0.699\n",
      "Step 660. time since epoch: 1916.789. Train acc: 0.781. Train Loss: 0.696\n",
      "Step 670. time since epoch: 1945.793. Train acc: 0.781. Train Loss: 0.694\n",
      "Step 680. time since epoch: 1974.894. Train acc: 0.782. Train Loss: 0.692\n",
      "Step 690. time since epoch: 2003.866. Train acc: 0.782. Train Loss: 0.689\n",
      "Step 700. time since epoch: 2032.823. Train acc: 0.783. Train Loss: 0.687\n",
      "Step 710. time since epoch: 2061.959. Train acc: 0.783. Train Loss: 0.686\n",
      "Step 720. time since epoch: 2091.019. Train acc: 0.784. Train Loss: 0.684\n",
      "Step 730. time since epoch: 2120.171. Train acc: 0.784. Train Loss: 0.682\n",
      "Step 740. time since epoch: 2149.314. Train acc: 0.785. Train Loss: 0.680\n",
      "Step 750. time since epoch: 2178.398. Train acc: 0.785. Train Loss: 0.679\n",
      "Step 760. time since epoch: 2207.396. Train acc: 0.785. Train Loss: 0.677\n",
      "Step 770. time since epoch: 2236.517. Train acc: 0.786. Train Loss: 0.675\n",
      "Step 780. time since epoch: 2265.434. Train acc: 0.787. Train Loss: 0.673\n",
      "Step 790. time since epoch: 2294.407. Train acc: 0.787. Train Loss: 0.671\n",
      "Step 800. time since epoch: 2323.437. Train acc: 0.787. Train Loss: 0.670\n",
      "Step 810. time since epoch: 2352.379. Train acc: 0.787. Train Loss: 0.668\n",
      "Step 820. time since epoch: 2381.371. Train acc: 0.788. Train Loss: 0.666\n",
      "Step 830. time since epoch: 2410.446. Train acc: 0.788. Train Loss: 0.665\n",
      "Step 840. time since epoch: 2439.534. Train acc: 0.788. Train Loss: 0.664\n",
      "Step 850. time since epoch: 2468.643. Train acc: 0.789. Train Loss: 0.662\n",
      "Step 860. time since epoch: 2497.858. Train acc: 0.789. Train Loss: 0.660\n",
      "Step 870. time since epoch: 2526.983. Train acc: 0.789. Train Loss: 0.658\n",
      "Step 880. time since epoch: 2556.042. Train acc: 0.790. Train Loss: 0.657\n",
      "Step 890. time since epoch: 2585.135. Train acc: 0.790. Train Loss: 0.655\n",
      "Step 900. time since epoch: 2614.195. Train acc: 0.791. Train Loss: 0.654\n",
      "Step 910. time since epoch: 2643.194. Train acc: 0.791. Train Loss: 0.653\n",
      "Step 920. time since epoch: 2672.342. Train acc: 0.791. Train Loss: 0.651\n",
      "Step 930. time since epoch: 2701.416. Train acc: 0.792. Train Loss: 0.650\n",
      "Step 940. time since epoch: 2730.457. Train acc: 0.792. Train Loss: 0.649\n",
      "Step 950. time since epoch: 2759.555. Train acc: 0.792. Train Loss: 0.647\n",
      "Step 960. time since epoch: 2788.670. Train acc: 0.792. Train Loss: 0.646\n",
      "Step 970. time since epoch: 2817.750. Train acc: 0.793. Train Loss: 0.645\n",
      "Step 980. time since epoch: 2846.847. Train acc: 0.793. Train Loss: 0.644\n",
      "Step 990. time since epoch: 2875.925. Train acc: 0.793. Train Loss: 0.643\n",
      "Step 1000. time since epoch: 2904.883. Train acc: 0.793. Train Loss: 0.642\n",
      "Step 1010. time since epoch: 2933.960. Train acc: 0.794. Train Loss: 0.641\n",
      "Step 1020. time since epoch: 2963.070. Train acc: 0.794. Train Loss: 0.640\n",
      "Step 1030. time since epoch: 2992.088. Train acc: 0.794. Train Loss: 0.639\n",
      "Step 1040. time since epoch: 3021.603. Train acc: 0.794. Train Loss: 0.638\n",
      "Step 1050. time since epoch: 3051.627. Train acc: 0.795. Train Loss: 0.637\n",
      "Step 1060. time since epoch: 3081.727. Train acc: 0.795. Train Loss: 0.635\n",
      "Step 1070. time since epoch: 3111.921. Train acc: 0.795. Train Loss: 0.634\n",
      "Step 1080. time since epoch: 3141.805. Train acc: 0.795. Train Loss: 0.634\n",
      "Step 1090. time since epoch: 3172.081. Train acc: 0.796. Train Loss: 0.633\n",
      "Step 1100. time since epoch: 3201.965. Train acc: 0.796. Train Loss: 0.632\n",
      "Step 1110. time since epoch: 3232.087. Train acc: 0.796. Train Loss: 0.631\n",
      "Step 1120. time since epoch: 3262.166. Train acc: 0.796. Train Loss: 0.630\n",
      "Step 1130. time since epoch: 3292.128. Train acc: 0.797. Train Loss: 0.629\n",
      "Step 1140. time since epoch: 3322.405. Train acc: 0.797. Train Loss: 0.629\n",
      "Step 1150. time since epoch: 3352.284. Train acc: 0.797. Train Loss: 0.627\n",
      "Step 1160. time since epoch: 3382.478. Train acc: 0.797. Train Loss: 0.627\n",
      "Step 1170. time since epoch: 3412.513. Train acc: 0.797. Train Loss: 0.626\n",
      "Step 1180. time since epoch: 3442.523. Train acc: 0.798. Train Loss: 0.625\n",
      "Step 1190. time since epoch: 3472.587. Train acc: 0.798. Train Loss: 0.624\n",
      "Step 1200. time since epoch: 3502.539. Train acc: 0.798. Train Loss: 0.623\n",
      "Step 1210. time since epoch: 3532.585. Train acc: 0.798. Train Loss: 0.622\n",
      "Step 1220. time since epoch: 3562.787. Train acc: 0.798. Train Loss: 0.621\n",
      "Step 1230. time since epoch: 3593.140. Train acc: 0.799. Train Loss: 0.620\n",
      "Step 1240. time since epoch: 3623.105. Train acc: 0.799. Train Loss: 0.620\n",
      "Step 1250. time since epoch: 3653.125. Train acc: 0.799. Train Loss: 0.619\n",
      "Step 1260. time since epoch: 3683.498. Train acc: 0.799. Train Loss: 0.618\n",
      "Step 1270. time since epoch: 3713.370. Train acc: 0.799. Train Loss: 0.618\n",
      "Step 1280. time since epoch: 3743.238. Train acc: 0.800. Train Loss: 0.617\n",
      "Step 1290. time since epoch: 3773.660. Train acc: 0.800. Train Loss: 0.617\n",
      "Step 1300. time since epoch: 3803.520. Train acc: 0.800. Train Loss: 0.616\n",
      "Step 1310. time since epoch: 3833.862. Train acc: 0.800. Train Loss: 0.615\n",
      "Step 1320. time since epoch: 3863.770. Train acc: 0.800. Train Loss: 0.614\n",
      "Step 1330. time since epoch: 3893.899. Train acc: 0.800. Train Loss: 0.614\n",
      "Step 1340. time since epoch: 3923.925. Train acc: 0.801. Train Loss: 0.613\n",
      "Step 1350. time since epoch: 3953.930. Train acc: 0.801. Train Loss: 0.612\n",
      "Step 1360. time since epoch: 3984.106. Train acc: 0.801. Train Loss: 0.612\n",
      "Step 1370. time since epoch: 4014.417. Train acc: 0.801. Train Loss: 0.611\n",
      "Step 1380. time since epoch: 4044.449. Train acc: 0.801. Train Loss: 0.610\n",
      "Step 1390. time since epoch: 4074.773. Train acc: 0.801. Train Loss: 0.610\n",
      "Step 1400. time since epoch: 4104.980. Train acc: 0.802. Train Loss: 0.609\n",
      "Step 1410. time since epoch: 4135.148. Train acc: 0.802. Train Loss: 0.609\n",
      "Step 1420. time since epoch: 4165.196. Train acc: 0.802. Train Loss: 0.608\n",
      "Step 1430. time since epoch: 4195.067. Train acc: 0.802. Train Loss: 0.607\n",
      "Step 1440. time since epoch: 4225.461. Train acc: 0.802. Train Loss: 0.607\n",
      "Step 1450. time since epoch: 4255.597. Train acc: 0.802. Train Loss: 0.606\n",
      "Step 1460. time since epoch: 4285.708. Train acc: 0.803. Train Loss: 0.606\n",
      "Step 1470. time since epoch: 4316.015. Train acc: 0.803. Train Loss: 0.605\n",
      "Step 1480. time since epoch: 4345.909. Train acc: 0.803. Train Loss: 0.605\n",
      "Step 1490. time since epoch: 4376.006. Train acc: 0.803. Train Loss: 0.604\n",
      "Step 1500. time since epoch: 4406.143. Train acc: 0.803. Train Loss: 0.604\n",
      "Step 1510. time since epoch: 4436.047. Train acc: 0.803. Train Loss: 0.603\n",
      "Step 1520. time since epoch: 4466.334. Train acc: 0.803. Train Loss: 0.603\n",
      "Step 1530. time since epoch: 4496.323. Train acc: 0.803. Train Loss: 0.602\n",
      "Step 1540. time since epoch: 4526.681. Train acc: 0.804. Train Loss: 0.602\n",
      "Step 1550. time since epoch: 4556.827. Train acc: 0.804. Train Loss: 0.601\n",
      "Step 1560. time since epoch: 4586.905. Train acc: 0.804. Train Loss: 0.601\n",
      "Step 1570. time since epoch: 4617.277. Train acc: 0.804. Train Loss: 0.600\n",
      "Step 1580. time since epoch: 4647.153. Train acc: 0.804. Train Loss: 0.600\n",
      "Step 1590. time since epoch: 4677.466. Train acc: 0.804. Train Loss: 0.599\n",
      "Step 1600. time since epoch: 4707.210. Train acc: 0.804. Train Loss: 0.599\n",
      "Step 1610. time since epoch: 4737.433. Train acc: 0.805. Train Loss: 0.599\n",
      "Step 1620. time since epoch: 4767.296. Train acc: 0.805. Train Loss: 0.598\n",
      "Step 1630. time since epoch: 4797.382. Train acc: 0.805. Train Loss: 0.597\n",
      "Step 1640. time since epoch: 4827.358. Train acc: 0.805. Train Loss: 0.597\n",
      "Step 1650. time since epoch: 4857.333. Train acc: 0.805. Train Loss: 0.597\n",
      "Step 1660. time since epoch: 4887.431. Train acc: 0.805. Train Loss: 0.596\n",
      "Step 1670. time since epoch: 4917.296. Train acc: 0.805. Train Loss: 0.596\n",
      "Step 1680. time since epoch: 4947.551. Train acc: 0.805. Train Loss: 0.595\n",
      "Step 1690. time since epoch: 4977.390. Train acc: 0.806. Train Loss: 0.595\n",
      "Step 1700. time since epoch: 5007.360. Train acc: 0.806. Train Loss: 0.594\n",
      "Step 1710. time since epoch: 5037.716. Train acc: 0.806. Train Loss: 0.594\n",
      "Step 1720. time since epoch: 5067.109. Train acc: 0.806. Train Loss: 0.594\n",
      "Step 1730. time since epoch: 5096.079. Train acc: 0.806. Train Loss: 0.593\n",
      "Step 1740. time since epoch: 5125.166. Train acc: 0.806. Train Loss: 0.593\n",
      "Step 1750. time since epoch: 5154.264. Train acc: 0.806. Train Loss: 0.592\n",
      "Step 1760. time since epoch: 5183.302. Train acc: 0.806. Train Loss: 0.592\n",
      "Step 1770. time since epoch: 5212.349. Train acc: 0.806. Train Loss: 0.591\n",
      "Step 1780. time since epoch: 5241.341. Train acc: 0.806. Train Loss: 0.592\n",
      "Step 1790. time since epoch: 5270.372. Train acc: 0.806. Train Loss: 0.591\n",
      "Step 1800. time since epoch: 5299.537. Train acc: 0.806. Train Loss: 0.591\n",
      "Step 1810. time since epoch: 5328.584. Train acc: 0.806. Train Loss: 0.591\n",
      "Step 1820. time since epoch: 5357.533. Train acc: 0.806. Train Loss: 0.590\n",
      "Step 1830. time since epoch: 5386.584. Train acc: 0.807. Train Loss: 0.590\n",
      "Step 1840. time since epoch: 5415.632. Train acc: 0.807. Train Loss: 0.590\n",
      "Step 1850. time since epoch: 5444.744. Train acc: 0.807. Train Loss: 0.589\n",
      "Step 1860. time since epoch: 5473.868. Train acc: 0.807. Train Loss: 0.589\n",
      "Step 1870. time since epoch: 5502.937. Train acc: 0.807. Train Loss: 0.589\n",
      "Step 1880. time since epoch: 5531.983. Train acc: 0.807. Train Loss: 0.589\n",
      "Step 1890. time since epoch: 5561.026. Train acc: 0.807. Train Loss: 0.588\n",
      "Step 1900. time since epoch: 5589.971. Train acc: 0.807. Train Loss: 0.588\n",
      "Step 1910. time since epoch: 5619.126. Train acc: 0.807. Train Loss: 0.587\n",
      "Step 1920. time since epoch: 5648.239. Train acc: 0.807. Train Loss: 0.587\n",
      "Step 1930. time since epoch: 5677.355. Train acc: 0.807. Train Loss: 0.587\n",
      "Step 1940. time since epoch: 5706.482. Train acc: 0.808. Train Loss: 0.586\n",
      "Step 1950. time since epoch: 5735.482. Train acc: 0.808. Train Loss: 0.586\n",
      "Step 1960. time since epoch: 5764.459. Train acc: 0.808. Train Loss: 0.586\n",
      "Step 1970. time since epoch: 5793.454. Train acc: 0.808. Train Loss: 0.585\n",
      "Step 1980. time since epoch: 5822.493. Train acc: 0.808. Train Loss: 0.585\n",
      "Step 1990. time since epoch: 5851.549. Train acc: 0.808. Train Loss: 0.585\n",
      "Step 2000. time since epoch: 5880.484. Train acc: 0.808. Train Loss: 0.584\n",
      "Step 2010. time since epoch: 5909.600. Train acc: 0.808. Train Loss: 0.584\n",
      "Step 2020. time since epoch: 5938.663. Train acc: 0.808. Train Loss: 0.584\n",
      "Step 2030. time since epoch: 5967.685. Train acc: 0.808. Train Loss: 0.583\n",
      "Step 2040. time since epoch: 5996.748. Train acc: 0.809. Train Loss: 0.583\n",
      "Step 2050. time since epoch: 6025.926. Train acc: 0.809. Train Loss: 0.582\n",
      "Step 2060. time since epoch: 6054.956. Train acc: 0.809. Train Loss: 0.582\n",
      "Step 2070. time since epoch: 6084.025. Train acc: 0.809. Train Loss: 0.581\n",
      "Step 2080. time since epoch: 6113.034. Train acc: 0.809. Train Loss: 0.581\n",
      "Step 2090. time since epoch: 6141.940. Train acc: 0.809. Train Loss: 0.581\n",
      "Step 2100. time since epoch: 6171.036. Train acc: 0.809. Train Loss: 0.581\n",
      "Step 2110. time since epoch: 6200.077. Train acc: 0.809. Train Loss: 0.580\n",
      "Step 2120. time since epoch: 6229.120. Train acc: 0.809. Train Loss: 0.580\n",
      "Step 2130. time since epoch: 6258.040. Train acc: 0.809. Train Loss: 0.580\n",
      "Step 2140. time since epoch: 6287.167. Train acc: 0.810. Train Loss: 0.579\n",
      "Step 2150. time since epoch: 6316.218. Train acc: 0.810. Train Loss: 0.579\n",
      "Step 2160. time since epoch: 6345.243. Train acc: 0.810. Train Loss: 0.578\n",
      "Step 2170. time since epoch: 6374.326. Train acc: 0.810. Train Loss: 0.578\n",
      "Step 2180. time since epoch: 6403.409. Train acc: 0.810. Train Loss: 0.578\n",
      "Step 2190. time since epoch: 6432.464. Train acc: 0.810. Train Loss: 0.577\n",
      "Step 2200. time since epoch: 6461.614. Train acc: 0.810. Train Loss: 0.577\n",
      "Step 2210. time since epoch: 6490.561. Train acc: 0.810. Train Loss: 0.577\n",
      "Step 2220. time since epoch: 6519.459. Train acc: 0.810. Train Loss: 0.577\n",
      "Step 2230. time since epoch: 6548.494. Train acc: 0.810. Train Loss: 0.576\n",
      "Step 2240. time since epoch: 6577.505. Train acc: 0.810. Train Loss: 0.576\n",
      "Step 2250. time since epoch: 6606.555. Train acc: 0.810. Train Loss: 0.576\n",
      "Step 2260. time since epoch: 6635.604. Train acc: 0.810. Train Loss: 0.576\n",
      "Step 2270. time since epoch: 6664.637. Train acc: 0.810. Train Loss: 0.575\n",
      "Step 2280. time since epoch: 6693.688. Train acc: 0.810. Train Loss: 0.575\n",
      "Step 2290. time since epoch: 6722.712. Train acc: 0.811. Train Loss: 0.575\n",
      "Step 2300. time since epoch: 6751.771. Train acc: 0.811. Train Loss: 0.575\n",
      "Step 2310. time since epoch: 6780.817. Train acc: 0.811. Train Loss: 0.574\n",
      "Step 2320. time since epoch: 6809.858. Train acc: 0.811. Train Loss: 0.574\n",
      "Step 2330. time since epoch: 6838.872. Train acc: 0.811. Train Loss: 0.574\n",
      "Step 2340. time since epoch: 6868.426. Train acc: 0.811. Train Loss: 0.574\n",
      "Step 2350. time since epoch: 6898.195. Train acc: 0.811. Train Loss: 0.573\n",
      "Step 2360. time since epoch: 6928.033. Train acc: 0.811. Train Loss: 0.573\n",
      "Step 2370. time since epoch: 6958.390. Train acc: 0.811. Train Loss: 0.573\n",
      "Step 2380. time since epoch: 6988.242. Train acc: 0.811. Train Loss: 0.573\n",
      "Step 2390. time since epoch: 7018.539. Train acc: 0.811. Train Loss: 0.572\n",
      "Step 2400. time since epoch: 7048.315. Train acc: 0.811. Train Loss: 0.572\n",
      "Step 2410. time since epoch: 7078.368. Train acc: 0.811. Train Loss: 0.572\n",
      "Step 2420. time since epoch: 7108.566. Train acc: 0.811. Train Loss: 0.572\n",
      "Step 2430. time since epoch: 7138.426. Train acc: 0.812. Train Loss: 0.571\n",
      "Step 2440. time since epoch: 7168.795. Train acc: 0.812. Train Loss: 0.571\n",
      "Step 2450. time since epoch: 7198.657. Train acc: 0.812. Train Loss: 0.571\n",
      "Step 2460. time since epoch: 7228.763. Train acc: 0.812. Train Loss: 0.571\n",
      "Step 2470. time since epoch: 7258.850. Train acc: 0.812. Train Loss: 0.571\n",
      "Step 2480. time since epoch: 7288.781. Train acc: 0.812. Train Loss: 0.570\n",
      "Step 2490. time since epoch: 7319.019. Train acc: 0.812. Train Loss: 0.570\n",
      "Step 2500. time since epoch: 7348.852. Train acc: 0.812. Train Loss: 0.570\n",
      "Step 2510. time since epoch: 7379.066. Train acc: 0.812. Train Loss: 0.570\n",
      "Step 2520. time since epoch: 7408.984. Train acc: 0.812. Train Loss: 0.570\n",
      "Step 2530. time since epoch: 7439.049. Train acc: 0.812. Train Loss: 0.569\n",
      "Step 2540. time since epoch: 7469.137. Train acc: 0.812. Train Loss: 0.569\n",
      "Step 2550. time since epoch: 7499.249. Train acc: 0.812. Train Loss: 0.569\n",
      "Step 2560. time since epoch: 7529.623. Train acc: 0.812. Train Loss: 0.569\n",
      "Step 2570. time since epoch: 7559.935. Train acc: 0.812. Train Loss: 0.568\n",
      "Step 2580. time since epoch: 7589.911. Train acc: 0.812. Train Loss: 0.568\n",
      "Step 2590. time since epoch: 7619.818. Train acc: 0.812. Train Loss: 0.568\n",
      "Step 2600. time since epoch: 7650.093. Train acc: 0.812. Train Loss: 0.568\n",
      "Step 2610. time since epoch: 7679.908. Train acc: 0.812. Train Loss: 0.567\n",
      "Step 2620. time since epoch: 7709.903. Train acc: 0.812. Train Loss: 0.567\n",
      "Step 2630. time since epoch: 7740.044. Train acc: 0.813. Train Loss: 0.567\n",
      "Step 2640. time since epoch: 7769.928. Train acc: 0.813. Train Loss: 0.567\n",
      "Step 2650. time since epoch: 7799.884. Train acc: 0.813. Train Loss: 0.567\n",
      "Step 2660. time since epoch: 7830.219. Train acc: 0.813. Train Loss: 0.567\n",
      "Step 2670. time since epoch: 7860.413. Train acc: 0.813. Train Loss: 0.566\n",
      "Step 2680. time since epoch: 7890.521. Train acc: 0.813. Train Loss: 0.566\n",
      "Step 2690. time since epoch: 7920.659. Train acc: 0.813. Train Loss: 0.566\n",
      "Step 2700. time since epoch: 7950.990. Train acc: 0.813. Train Loss: 0.566\n",
      "Step 2710. time since epoch: 7981.071. Train acc: 0.813. Train Loss: 0.566\n",
      "Step 2720. time since epoch: 8011.415. Train acc: 0.813. Train Loss: 0.565\n",
      "--------------------\n",
      "epoch 1, loss 0.5654, train acc 0.813, test acc 0.855, time 9314.0 sec\n"
     ]
    }
   ],
   "source": [
    "train(model, train_iter, test_iter, trainer, 1, list_of_losses=vgg_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "transoforms = tv.transforms.Compose([\n",
    "    tv.transforms.Grayscale(num_output_channels=3),\n",
    "    tv.transforms.Resize((299, 299)),\n",
    "    tv.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = tv.datasets.EMNIST('.', split='byclass', train=True, transform=transoforms, download=True)\n",
    "test_dataset = tv.datasets.EMNIST('.', split='byclass', train=False, transform=transoforms, download=True)\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, trainer, num_epochs, list_of_losses):\n",
    "    net.to(device)\n",
    "    loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        \n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            trainer.zero_grad()\n",
    "            outputs = net(X)\n",
    "            \n",
    "            if isinstance(outputs, tuple):\n",
    "                y_hat, aux_y_hat = outputs  \n",
    "                l1 = loss(y_hat, y)  \n",
    "                l2 = loss(aux_y_hat, y)  \n",
    "                l = l1 + 0.4 * l2  \n",
    "            else:\n",
    "                l = loss(outputs, y)\n",
    "\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "\n",
    "            if i % 10 == 0:\n",
    "              print(f\"Step {i}. time since epoch: {time.time() -  start:.3f}. \" \n",
    "                    f\"Train acc: {train_acc_sum / n:.3f}. Train Loss: {train_l_sum / n:.3f}\")\n",
    "            list_of_losses.append(round(train_l_sum / n, 3))\n",
    "        test_acc = evaluate_accuracy(test_iter, net.to(device))\n",
    "        print('-' * 20)\n",
    "        print(f'epoch {epoch + 1}, loss {train_l_sum / n:.4f}, train acc {train_acc_sum / n:.3f}'\n",
    "              f', test acc {test_acc:.3f}, time {time.time() - start:.1f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kosar\\anaconda3\\envs\\mult_nn_net\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = tv.models.inception_v3(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(in_features=model.fc.in_features, out_features=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.Adam(params_to_update, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0. time since epoch: 0.606. Train acc: 0.016. Train Loss: 7.477\n",
      "Step 10. time since epoch: 6.362. Train acc: 0.018. Train Loss: 7.419\n",
      "Step 20. time since epoch: 11.988. Train acc: 0.017. Train Loss: 7.403\n",
      "Step 30. time since epoch: 17.615. Train acc: 0.016. Train Loss: 7.396\n",
      "Step 40. time since epoch: 23.319. Train acc: 0.018. Train Loss: 7.395\n",
      "Step 50. time since epoch: 28.981. Train acc: 0.018. Train Loss: 7.392\n",
      "Step 60. time since epoch: 34.602. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 70. time since epoch: 40.196. Train acc: 0.018. Train Loss: 7.395\n",
      "Step 80. time since epoch: 45.827. Train acc: 0.018. Train Loss: 7.392\n",
      "Step 90. time since epoch: 51.506. Train acc: 0.018. Train Loss: 7.392\n",
      "Step 100. time since epoch: 57.200. Train acc: 0.018. Train Loss: 7.392\n",
      "Step 110. time since epoch: 62.844. Train acc: 0.018. Train Loss: 7.390\n",
      "Step 120. time since epoch: 68.588. Train acc: 0.019. Train Loss: 7.390\n",
      "Step 130. time since epoch: 74.274. Train acc: 0.019. Train Loss: 7.388\n",
      "Step 140. time since epoch: 80.106. Train acc: 0.019. Train Loss: 7.391\n",
      "Step 150. time since epoch: 85.799. Train acc: 0.019. Train Loss: 7.391\n",
      "Step 160. time since epoch: 91.456. Train acc: 0.019. Train Loss: 7.391\n",
      "Step 170. time since epoch: 97.093. Train acc: 0.018. Train Loss: 7.391\n",
      "Step 180. time since epoch: 102.775. Train acc: 0.018. Train Loss: 7.390\n",
      "Step 190. time since epoch: 108.393. Train acc: 0.019. Train Loss: 7.390\n",
      "Step 200. time since epoch: 113.945. Train acc: 0.019. Train Loss: 7.390\n",
      "Step 210. time since epoch: 119.575. Train acc: 0.019. Train Loss: 7.390\n",
      "Step 220. time since epoch: 125.270. Train acc: 0.019. Train Loss: 7.389\n",
      "Step 230. time since epoch: 130.937. Train acc: 0.019. Train Loss: 7.391\n",
      "Step 240. time since epoch: 136.649. Train acc: 0.019. Train Loss: 7.391\n",
      "Step 250. time since epoch: 142.257. Train acc: 0.019. Train Loss: 7.391\n",
      "Step 260. time since epoch: 147.967. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 270. time since epoch: 153.604. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 280. time since epoch: 159.264. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 290. time since epoch: 164.842. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 300. time since epoch: 170.434. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 310. time since epoch: 176.068. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 320. time since epoch: 181.701. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 330. time since epoch: 187.307. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 340. time since epoch: 192.878. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 350. time since epoch: 198.695. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 360. time since epoch: 204.318. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 370. time since epoch: 209.952. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 380. time since epoch: 215.502. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 390. time since epoch: 221.039. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 400. time since epoch: 226.706. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 410. time since epoch: 232.380. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 420. time since epoch: 237.945. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 430. time since epoch: 243.519. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 440. time since epoch: 249.112. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 450. time since epoch: 254.767. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 460. time since epoch: 260.308. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 470. time since epoch: 266.065. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 480. time since epoch: 271.664. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 490. time since epoch: 277.307. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 500. time since epoch: 282.973. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 510. time since epoch: 288.755. Train acc: 0.019. Train Loss: 7.391\n",
      "Step 520. time since epoch: 294.436. Train acc: 0.019. Train Loss: 7.391\n",
      "Step 530. time since epoch: 300.081. Train acc: 0.019. Train Loss: 7.391\n",
      "Step 540. time since epoch: 305.717. Train acc: 0.019. Train Loss: 7.391\n",
      "Step 550. time since epoch: 311.346. Train acc: 0.019. Train Loss: 7.391\n",
      "Step 560. time since epoch: 317.010. Train acc: 0.019. Train Loss: 7.391\n",
      "Step 570. time since epoch: 322.698. Train acc: 0.019. Train Loss: 7.391\n",
      "Step 580. time since epoch: 328.440. Train acc: 0.019. Train Loss: 7.391\n",
      "Step 590. time since epoch: 334.171. Train acc: 0.019. Train Loss: 7.391\n",
      "Step 600. time since epoch: 339.756. Train acc: 0.019. Train Loss: 7.391\n",
      "Step 610. time since epoch: 345.365. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 620. time since epoch: 350.962. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 630. time since epoch: 356.598. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 640. time since epoch: 362.411. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 650. time since epoch: 368.040. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 660. time since epoch: 373.753. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 670. time since epoch: 379.487. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 680. time since epoch: 385.139. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 690. time since epoch: 390.871. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 700. time since epoch: 396.493. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 710. time since epoch: 402.046. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 720. time since epoch: 407.637. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 730. time since epoch: 413.343. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 740. time since epoch: 418.987. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 750. time since epoch: 424.661. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 760. time since epoch: 430.339. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 770. time since epoch: 436.017. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 780. time since epoch: 441.660. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 790. time since epoch: 447.328. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 800. time since epoch: 452.951. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 810. time since epoch: 458.519. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 820. time since epoch: 464.154. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 830. time since epoch: 469.750. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 840. time since epoch: 475.430. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 850. time since epoch: 481.088. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 860. time since epoch: 486.749. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 870. time since epoch: 492.438. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 880. time since epoch: 498.063. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 890. time since epoch: 503.655. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 900. time since epoch: 509.244. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 910. time since epoch: 515.003. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 920. time since epoch: 520.709. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 930. time since epoch: 526.478. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 940. time since epoch: 532.192. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 950. time since epoch: 537.879. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 960. time since epoch: 543.543. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 970. time since epoch: 549.061. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 980. time since epoch: 554.656. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 990. time since epoch: 560.191. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 1000. time since epoch: 565.819. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 1010. time since epoch: 571.428. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 1020. time since epoch: 577.012. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 1030. time since epoch: 582.622. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 1040. time since epoch: 588.255. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 1050. time since epoch: 593.840. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 1060. time since epoch: 599.562. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 1070. time since epoch: 605.083. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 1080. time since epoch: 610.683. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 1090. time since epoch: 616.278. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 1100. time since epoch: 621.823. Train acc: 0.019. Train Loss: 7.392\n",
      "Step 1110. time since epoch: 627.300. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1120. time since epoch: 632.882. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1130. time since epoch: 638.477. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1140. time since epoch: 644.015. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1150. time since epoch: 649.638. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1160. time since epoch: 655.150. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1170. time since epoch: 660.744. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1180. time since epoch: 666.349. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1190. time since epoch: 672.050. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1200. time since epoch: 677.694. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1210. time since epoch: 683.234. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1220. time since epoch: 688.758. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1230. time since epoch: 694.328. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1240. time since epoch: 699.903. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1250. time since epoch: 705.493. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1260. time since epoch: 711.091. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1270. time since epoch: 716.675. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1280. time since epoch: 722.206. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1290. time since epoch: 727.843. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1300. time since epoch: 733.443. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1310. time since epoch: 739.122. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1320. time since epoch: 744.651. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1330. time since epoch: 750.185. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1340. time since epoch: 755.655. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1350. time since epoch: 761.157. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1360. time since epoch: 766.655. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1370. time since epoch: 772.128. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1380. time since epoch: 777.704. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1390. time since epoch: 783.262. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1400. time since epoch: 788.871. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1410. time since epoch: 794.422. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1420. time since epoch: 799.902. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1430. time since epoch: 805.431. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1440. time since epoch: 810.935. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1450. time since epoch: 816.449. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1460. time since epoch: 821.975. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1470. time since epoch: 827.611. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1480. time since epoch: 833.062. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1490. time since epoch: 838.498. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1500. time since epoch: 844.087. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1510. time since epoch: 849.700. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1520. time since epoch: 855.283. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1530. time since epoch: 860.900. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1540. time since epoch: 866.476. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1550. time since epoch: 872.006. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1560. time since epoch: 877.729. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1570. time since epoch: 883.292. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1580. time since epoch: 888.887. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1590. time since epoch: 894.434. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1600. time since epoch: 900.055. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1610. time since epoch: 905.643. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1620. time since epoch: 911.111. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1630. time since epoch: 916.759. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1640. time since epoch: 922.289. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1650. time since epoch: 927.992. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1660. time since epoch: 933.645. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1670. time since epoch: 939.265. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1680. time since epoch: 944.881. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1690. time since epoch: 950.485. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1700. time since epoch: 956.065. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1710. time since epoch: 961.617. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1720. time since epoch: 967.238. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1730. time since epoch: 972.803. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1740. time since epoch: 978.379. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1750. time since epoch: 983.995. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1760. time since epoch: 989.596. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1770. time since epoch: 995.184. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1780. time since epoch: 1000.867. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1790. time since epoch: 1006.442. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1800. time since epoch: 1012.033. Train acc: 0.019. Train Loss: 7.393\n",
      "Step 1810. time since epoch: 1017.529. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 1820. time since epoch: 1023.086. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 1830. time since epoch: 1028.664. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 1840. time since epoch: 1034.268. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 1850. time since epoch: 1039.893. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 1860. time since epoch: 1045.368. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 1870. time since epoch: 1050.945. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 1880. time since epoch: 1056.540. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 1890. time since epoch: 1062.182. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 1900. time since epoch: 1067.873. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 1910. time since epoch: 1073.497. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 1920. time since epoch: 1079.036. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 1930. time since epoch: 1084.564. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 1940. time since epoch: 1090.161. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 1950. time since epoch: 1095.658. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 1960. time since epoch: 1101.277. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 1970. time since epoch: 1106.844. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 1980. time since epoch: 1112.392. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 1990. time since epoch: 1117.920. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2000. time since epoch: 1123.419. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2010. time since epoch: 1129.049. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2020. time since epoch: 1134.663. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2030. time since epoch: 1140.317. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2040. time since epoch: 1145.958. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2050. time since epoch: 1151.464. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2060. time since epoch: 1156.935. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2070. time since epoch: 1162.452. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2080. time since epoch: 1167.974. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2090. time since epoch: 1173.473. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2100. time since epoch: 1179.135. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2110. time since epoch: 1184.666. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2120. time since epoch: 1190.282. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2130. time since epoch: 1195.884. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2140. time since epoch: 1201.423. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2150. time since epoch: 1206.982. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2160. time since epoch: 1212.476. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2170. time since epoch: 1218.028. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2180. time since epoch: 1223.549. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2190. time since epoch: 1229.110. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2200. time since epoch: 1234.684. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2210. time since epoch: 1240.274. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2220. time since epoch: 1245.844. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2230. time since epoch: 1251.493. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2240. time since epoch: 1257.182. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2250. time since epoch: 1262.783. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2260. time since epoch: 1268.285. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2270. time since epoch: 1273.809. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2280. time since epoch: 1279.420. Train acc: 0.019. Train Loss: 7.394\n",
      "Step 2290. time since epoch: 1285.021. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2300. time since epoch: 1290.587. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2310. time since epoch: 1296.199. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2320. time since epoch: 1301.831. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2330. time since epoch: 1307.348. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2340. time since epoch: 1312.923. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2350. time since epoch: 1318.429. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2360. time since epoch: 1324.051. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2370. time since epoch: 1329.753. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2380. time since epoch: 1335.374. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2390. time since epoch: 1341.021. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2400. time since epoch: 1346.614. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2410. time since epoch: 1352.192. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2420. time since epoch: 1357.734. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2430. time since epoch: 1363.277. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2440. time since epoch: 1368.868. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2450. time since epoch: 1374.441. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2460. time since epoch: 1379.997. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2470. time since epoch: 1385.575. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2480. time since epoch: 1391.170. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2490. time since epoch: 1396.820. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2500. time since epoch: 1402.499. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2510. time since epoch: 1408.158. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2520. time since epoch: 1413.683. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2530. time since epoch: 1419.298. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2540. time since epoch: 1424.815. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2550. time since epoch: 1430.378. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2560. time since epoch: 1435.915. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2570. time since epoch: 1441.483. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2580. time since epoch: 1446.983. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2590. time since epoch: 1452.541. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2600. time since epoch: 1458.153. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2610. time since epoch: 1463.767. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2620. time since epoch: 1469.372. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2630. time since epoch: 1474.903. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2640. time since epoch: 1480.461. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2650. time since epoch: 1485.986. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2660. time since epoch: 1491.488. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2670. time since epoch: 1497.038. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2680. time since epoch: 1502.526. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2690. time since epoch: 1508.065. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2700. time since epoch: 1513.561. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2710. time since epoch: 1519.149. Train acc: 0.018. Train Loss: 7.394\n",
      "Step 2720. time since epoch: 1524.786. Train acc: 0.018. Train Loss: 7.394\n",
      "--------------------\n",
      "epoch 1, loss 7.3938, train acc 0.018, test acc 0.013, time 1660.9 sec\n"
     ]
    }
   ],
   "source": [
    "train(model, train_iter, test_iter, trainer, 1, list_of_losses=inception_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2048, out_features=62, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### График лоссов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.plot(res_losses, label='ResNet18', linewidth=2)\n",
    "plt.plot(dense_losses, label='DenseNet161', linewidth=2)\n",
    "plt.plot(vgg_losses, label='VGG16', linewidth=2)\n",
    "plt.plot(inception_losses, label='InceptionV3', linewidth=2)\n",
    "\n",
    "plt.title('Сравнение лоссов моделей', fontsize=16)\n",
    "plt.xlabel('Steps', fontsize=14)\n",
    "plt.ylabel('Training Loss', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Модель</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DenseNet161</td>\n",
       "      <td>0.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ResNet18</td>\n",
       "      <td>0.802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VGG16</td>\n",
       "      <td>0.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inception v3</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Модель  Test Accuracy\n",
       "0   DenseNet161          0.806\n",
       "1      ResNet18          0.802\n",
       "2         VGG16          0.855\n",
       "3  Inception v3          0.013"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = {'Модель': ['DenseNet161', 'ResNet18', 'VGG16', 'Inception v3'], 'Test Accuracy': [0.806, 0.802, 0.855, 0.013]}\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mult_nn_net",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
